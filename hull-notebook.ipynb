{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcb0d8f",
   "metadata": {
    "papermill": {
     "duration": 0.006075,
     "end_time": "2025-12-01T16:00:07.911048",
     "exception": false,
     "start_time": "2025-12-01T16:00:07.904973",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea608f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:07.922618Z",
     "iopub.status.busy": "2025-12-01T16:00:07.922254Z",
     "iopub.status.idle": "2025-12-01T16:00:12.797829Z",
     "shell.execute_reply": "2025-12-01T16:00:12.796390Z"
    },
    "papermill": {
     "duration": 4.883333,
     "end_time": "2025-12-01T16:00:12.799695",
     "exception": false,
     "start_time": "2025-12-01T16:00:07.916362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Imports & Global Setup\n",
    "# ============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.auto import tqdm  # lebih aman di notebook\n",
    "\n",
    "import kaggle_evaluation.default_inference_server as kei\n",
    "# nantinya dipakai sebagai: kei.DefaultInferenceServer(...)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Reproducibility\n",
    "# ============================================================\n",
    "SEED: int = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# optional: kalau nanti pakai model lain yang ada random_state, pakai SEED ini\n",
    "\n",
    "# ============================================================\n",
    "# 3. Warning / Logging Setup (minimal)\n",
    "# ============================================================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab74cb5",
   "metadata": {
    "papermill": {
     "duration": 0.004498,
     "end_time": "2025-12-01T16:00:12.809299",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.804801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb9e8f7",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:12.820330Z",
     "iopub.status.busy": "2025-12-01T16:00:12.819843Z",
     "iopub.status.idle": "2025-12-01T16:00:12.833586Z",
     "shell.execute_reply": "2025-12-01T16:00:12.832347Z"
    },
    "papermill": {
     "duration": 0.021664,
     "end_time": "2025-12-01T16:00:12.835598",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.813934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hull-tactical-market-prediction/train.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/test.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb2ba4b",
   "metadata": {
    "papermill": {
     "duration": 0.005637,
     "end_time": "2025-12-01T16:00:12.846569",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.840932",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf67c3a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:12.857894Z",
     "iopub.status.busy": "2025-12-01T16:00:12.857483Z",
     "iopub.status.idle": "2025-12-01T16:00:12.873624Z",
     "shell.execute_reply": "2025-12-01T16:00:12.872507Z"
    },
    "papermill": {
     "duration": 0.023927,
     "end_time": "2025-12-01T16:00:12.875490",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.851563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & GLOBAL SETUP\n",
    "# ============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import kaggle_evaluation.default_inference_server as kei\n",
    "\n",
    "# Reproducibility\n",
    "SEED: int = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# Sedikit bersihin warning yang kurang penting\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. PROJECT DIRECTORY STRUCTURE & CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Nama kompetisi (dipakai untuk bikin folder kerja terpisah)\n",
    "COMP_NAME: str = \"hull-tactical-market-prediction\"\n",
    "\n",
    "# ---- Input & Working Dirs ----\n",
    "INPUT_DIR: Path = Path(\"/kaggle/input\") / COMP_NAME\n",
    "WORK_DIR: Path  = Path(\"/kaggle/working\") / COMP_NAME\n",
    "\n",
    "# Pastikan WORK_DIR ada (supaya semua output ngumpul di sini)\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File data utama\n",
    "TRAIN_PATH: Path = INPUT_DIR / \"train.csv\"\n",
    "TEST_PATH: Path  = INPUT_DIR / \"test.csv\"\n",
    "\n",
    "# Folder resmi dari Kaggle Evaluation API (source & copy ke working jika perlu)\n",
    "KAGGLE_EVAL_SRC: Path  = INPUT_DIR / \"kaggle_evaluation\"\n",
    "KAGGLE_EVAL_WORK: Path = WORK_DIR / \"kaggle_evaluation\"\n",
    "KAGGLE_EVAL_WORK.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Output structure (untuk hasil eksperimen) ----\n",
    "OUT_DIR: Path        = WORK_DIR / \"outputs\"\n",
    "MODEL_DIR: Path      = OUT_DIR / \"models\"       # simpan model, scaler, dsb.\n",
    "FEATURE_DIR: Path    = OUT_DIR / \"features\"     # simpan dataset hasil FE (opsional)\n",
    "LOG_DIR: Path        = OUT_DIR / \"logs\"         # catatan eksperimen, metrik\n",
    "SUBMISSION_DIR: Path = OUT_DIR / \"submissions\"  # submission lokal untuk dicek\n",
    "\n",
    "for p in [OUT_DIR, MODEL_DIR, FEATURE_DIR, LOG_DIR, SUBMISSION_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Flag environment: apakah ini run dalam mode kompetisi (rerun) atau lokal\n",
    "IS_COMP_RERUN: bool = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") is not None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. RETURNS -> SIGNAL CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SignalConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi untuk mengubah prediksi return -> sinyal trading harian.\n",
    "    \"\"\"\n",
    "    min_signal: float = 0.0      # posisi minimum (0 = full cash)\n",
    "    max_signal: float = 2.0      # posisi maksimum (2 = 2x leverage)\n",
    "    multiplier: float = 400.0    # pengali untuk mengeskalasi return -> posisi\n",
    "\n",
    "SIGNAL_CFG = SignalConfig()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. MODEL CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi utama untuk model ElasticNet baseline.\n",
    "    \"\"\"\n",
    "    cv_folds: int = 10\n",
    "    l1_ratio: float = 0.5\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: np.logspace(-4, 2, 100)\n",
    "    )\n",
    "    max_iter: int = 1_000_000\n",
    "\n",
    "ENET_CFG = ModelConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f87aa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:12.887172Z",
     "iopub.status.busy": "2025-12-01T16:00:12.886398Z",
     "iopub.status.idle": "2025-12-01T16:00:12.895459Z",
     "shell.execute_reply": "2025-12-01T16:00:12.894087Z"
    },
    "papermill": {
     "duration": 0.016613,
     "end_time": "2025-12-01T16:00:12.897021",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.880408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR : /kaggle/working/hull-tactical-market-prediction\n",
      "TRAIN_PATH exists: True\n",
      "TEST_PATH exists : True\n",
      "Signal config    : SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=400.0)\n",
      "Model config     : ModelConfig(cv_folds=10, l1_ratio=0.5, alphas=array([1.00000000e-04, 1.14975700e-04, 1.32194115e-04, 1.51991108e-04,\n",
      "       1.74752840e-04, 2.00923300e-04, 2.31012970e-04, 2.65608778e-04,\n",
      "       3.05385551e-04, 3.51119173e-04, 4.03701726e-04, 4.64158883e-04,\n",
      "       5.33669923e-04, 6.13590727e-04, 7.05480231e-04, 8.11130831e-04,\n",
      "       9.32603347e-04, 1.07226722e-03, 1.23284674e-03, 1.41747416e-03,\n",
      "       1.62975083e-03, 1.87381742e-03, 2.15443469e-03, 2.47707636e-03,\n",
      "       2.84803587e-03, 3.27454916e-03, 3.76493581e-03, 4.32876128e-03,\n",
      "       4.97702356e-03, 5.72236766e-03, 6.57933225e-03, 7.56463328e-03,\n",
      "       8.69749003e-03, 1.00000000e-02, 1.14975700e-02, 1.32194115e-02,\n",
      "       1.51991108e-02, 1.74752840e-02, 2.00923300e-02, 2.31012970e-02,\n",
      "       2.65608778e-02, 3.05385551e-02, 3.51119173e-02, 4.03701726e-02,\n",
      "       4.64158883e-02, 5.33669923e-02, 6.13590727e-02, 7.05480231e-02,\n",
      "       8.11130831e-02, 9.32603347e-02, 1.07226722e-01, 1.23284674e-01,\n",
      "       1.41747416e-01, 1.62975083e-01, 1.87381742e-01, 2.15443469e-01,\n",
      "       2.47707636e-01, 2.84803587e-01, 3.27454916e-01, 3.76493581e-01,\n",
      "       4.32876128e-01, 4.97702356e-01, 5.72236766e-01, 6.57933225e-01,\n",
      "       7.56463328e-01, 8.69749003e-01, 1.00000000e+00, 1.14975700e+00,\n",
      "       1.32194115e+00, 1.51991108e+00, 1.74752840e+00, 2.00923300e+00,\n",
      "       2.31012970e+00, 2.65608778e+00, 3.05385551e+00, 3.51119173e+00,\n",
      "       4.03701726e+00, 4.64158883e+00, 5.33669923e+00, 6.13590727e+00,\n",
      "       7.05480231e+00, 8.11130831e+00, 9.32603347e+00, 1.07226722e+01,\n",
      "       1.23284674e+01, 1.41747416e+01, 1.62975083e+01, 1.87381742e+01,\n",
      "       2.15443469e+01, 2.47707636e+01, 2.84803587e+01, 3.27454916e+01,\n",
      "       3.76493581e+01, 4.32876128e+01, 4.97702356e+01, 5.72236766e+01,\n",
      "       6.57933225e+01, 7.56463328e+01, 8.69749003e+01, 1.00000000e+02]), max_iter=1000000)\n"
     ]
    }
   ],
   "source": [
    "print(\"WORK_DIR :\", WORK_DIR)\n",
    "print(\"TRAIN_PATH exists:\", TRAIN_PATH.exists())\n",
    "print(\"TEST_PATH exists :\", TEST_PATH.exists())\n",
    "print(\"Signal config    :\", SIGNAL_CFG)\n",
    "print(\"Model config     :\", ENET_CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aeb36d",
   "metadata": {
    "papermill": {
     "duration": 0.004576,
     "end_time": "2025-12-01T16:00:12.906697",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.902121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataclasses Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0418787c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:12.919415Z",
     "iopub.status.busy": "2025-12-01T16:00:12.919093Z",
     "iopub.status.idle": "2025-12-01T16:00:12.931665Z",
     "shell.execute_reply": "2025-12-01T16:00:12.930587Z"
    },
    "papermill": {
     "duration": 0.020608,
     "end_time": "2025-12-01T16:00:12.933478",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.912870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. DATACLASSES HELPERS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    \"\"\"\n",
    "    Paket hasil preprocessing dataset.\n",
    "\n",
    "    - X_train, y_train : data untuk melatih model\n",
    "    - X_test,  y_test  : data untuk evaluasi (mock test / hold-out)\n",
    "    - scaler           : StandardScaler yang sudah di-fit pada X_train\n",
    "    - feature_names    : daftar nama fitur (opsional, tapi berguna untuk debugging)\n",
    "    \"\"\"\n",
    "    X_train: pl.DataFrame\n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "    feature_names: list[str] | None = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ElasticNetParameters:\n",
    "    \"\"\"\n",
    "    Parameter yang dipakai ketika membangun ElasticNetCV / ElasticNet.\n",
    "\n",
    "    Default-nya diambil dari ENET_CFG (ModelConfig) supaya konsisten\n",
    "    dengan konfigurasi global, tapi tetap bisa dioverride kalau perlu.\n",
    "    \"\"\"\n",
    "    l1_ratio: float = ENET_CFG.l1_ratio\n",
    "    cv: int = ENET_CFG.cv_folds\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: ENET_CFG.alphas.copy()\n",
    "    )\n",
    "    max_iter: int = ENET_CFG.max_iter\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not (0.0 <= self.l1_ratio <= 1.0):\n",
    "            raise ValueError(\n",
    "                \"ElasticNet l1_ratio harus berada di dalam interval [0, 1].\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    Default diambil dari SIGNAL_CFG supaya selaras dengan konfigurasi global.\n",
    "    \"\"\"\n",
    "    signal_multiplier: float = SIGNAL_CFG.multiplier\n",
    "    min_signal: float = SIGNAL_CFG.min_signal\n",
    "    max_signal: float = SIGNAL_CFG.max_signal\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.min_signal >= self.max_signal:\n",
    "            raise ValueError(\n",
    "                \"min_signal harus lebih kecil daripada max_signal.\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10249f50",
   "metadata": {
    "papermill": {
     "duration": 0.004623,
     "end_time": "2025-12-01T16:00:12.943311",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.938688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4284dfa5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:12.955255Z",
     "iopub.status.busy": "2025-12-01T16:00:12.954944Z",
     "iopub.status.idle": "2025-12-01T16:00:12.966830Z",
     "shell.execute_reply": "2025-12-01T16:00:12.965725Z"
    },
    "papermill": {
     "duration": 0.020142,
     "end_time": "2025-12-01T16:00:12.968445",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.948303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. DATACLASSES HELPERS + PARAMETER OBJECTS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    \"\"\"\n",
    "    Paket hasil preprocessing dataset.\n",
    "\n",
    "    - X_train, y_train : data untuk melatih model\n",
    "    - X_test,  y_test  : data untuk evaluasi (mock test / hold-out)\n",
    "    - scaler           : StandardScaler yang sudah di-fit pada X_train\n",
    "    - feature_names    : daftar nama fitur (opsional, berguna untuk debugging/analisis)\n",
    "    \"\"\"\n",
    "    X_train: pl.DataFrame\n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "    feature_names: list[str] | None = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ElasticNetParameters:\n",
    "    \"\"\"\n",
    "    Parameter yang dipakai ketika membangun ElasticNetCV / ElasticNet.\n",
    "\n",
    "    Default diambil dari ENET_CFG (ModelConfig) supaya konsisten dengan\n",
    "    konfigurasi global, tapi bisa dioverride kalau perlu.\n",
    "    \"\"\"\n",
    "    l1_ratio: float = ENET_CFG.l1_ratio\n",
    "    cv: int = ENET_CFG.cv_folds\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: ENET_CFG.alphas.copy()\n",
    "    )\n",
    "    max_iter: int = ENET_CFG.max_iter\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not (0.0 <= self.l1_ratio <= 1.0):\n",
    "            raise ValueError(\n",
    "                \"ElasticNet l1_ratio harus berada di dalam interval [0, 1].\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    Default diambil dari SIGNAL_CFG supaya selaras dengan konfigurasi global.\n",
    "    \"\"\"\n",
    "    signal_multiplier: float = SIGNAL_CFG.multiplier\n",
    "    min_signal: float = SIGNAL_CFG.min_signal\n",
    "    max_signal: float = SIGNAL_CFG.max_signal\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.min_signal >= self.max_signal:\n",
    "            raise ValueError(\n",
    "                \"min_signal harus lebih kecil daripada max_signal.\"\n",
    "            )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Instansiasi objek parameter yang akan dipakai di pipeline\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "ret_signal_params = RetToSignalParameters()\n",
    "enet_params       = ElasticNetParameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30653388",
   "metadata": {
    "papermill": {
     "duration": 0.004528,
     "end_time": "2025-12-01T16:00:12.978512",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.973984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Loading/Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d7f749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:12.989797Z",
     "iopub.status.busy": "2025-12-01T16:00:12.989432Z",
     "iopub.status.idle": "2025-12-01T16:00:13.006404Z",
     "shell.execute_reply": "2025-12-01T16:00:13.005347Z"
    },
    "papermill": {
     "duration": 0.024782,
     "end_time": "2025-12-01T16:00:13.008068",
     "exception": false,
     "start_time": "2025-12-01T16:00:12.983286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. DATA LOADING & PREPROCESSING HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def load_trainset(path: Path = TRAIN_PATH, drop_last_n: int = 10) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dan praproses training dataset.\n",
    "\n",
    "    - Mengganti nama kolom target menjadi 'target'\n",
    "      (market_forward_excess_returns -> target).\n",
    "    - Meng-cast semua kolom selain 'date_id' ke Float64.\n",
    "    - Opsional: membuang N baris terakhir (drop_last_n) untuk\n",
    "      menghindari kebocoran saat mock test.\n",
    "\n",
    "    Args:\n",
    "        path (Path): lokasi file train.csv.\n",
    "        drop_last_n (int): jumlah baris terakhir yang dibuang.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame training yang sudah di-cast dan terurut.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pl.read_csv(path)\n",
    "        .rename({\"market_forward_excess_returns\": \"target\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"date_id\").cast(pl.Int32, strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "    if drop_last_n > 0:\n",
    "        df = df.head(-drop_last_n)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_testset(path: Path = TEST_PATH) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dan praproses test/mock dataset.\n",
    "\n",
    "    - Mengganti nama 'lagged_forward_returns' -> 'target' agar\n",
    "      struktur mirip train (meski tidak dipakai sebagai ground truth).\n",
    "    - Meng-cast semua kolom selain 'date_id' ke Float64.\n",
    "\n",
    "    Args:\n",
    "        path (Path): lokasi file test.csv.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame test yang sudah di-cast dan terurut.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pl.read_csv(path)\n",
    "        .rename({\"lagged_forward_returns\": \"target\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"date_id\").cast(pl.Int32, strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Membuat fitur contoh (baseline) dan membersihkan DataFrame.\n",
    "\n",
    "    - Menambahkan dua fitur baru:\n",
    "        U1 = I2 - I1\n",
    "        U2 = M11 / mean(I2, I9, I7)\n",
    "    - Memilih subset fitur yang akan dipakai model.\n",
    "    - Mengisi missing value dengan exponential weighted mean (ewm).\n",
    "    - Menghapus baris yang masih mengandung null setelah imputasi.\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): input Polars DataFrame (train+test gabungan).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame dengan fitur baru, kolom terpilih,\n",
    "                      dan tanpa nilai null.\n",
    "    \"\"\"\n",
    "    vars_to_keep: List[str] = [\n",
    "        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "        \"P10\", \"P12\", \"P13\", \"U1\", \"U2\",\n",
    "    ]\n",
    "\n",
    "    # Sedikit sanity-check kolom penting\n",
    "    required_base_cols = [\"I1\", \"I2\", \"M11\", \"I7\", \"I9\"]\n",
    "    missing = [c for c in required_base_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Kolom berikut hilang di DataFrame: {missing}\")\n",
    "\n",
    "    df_feat = (\n",
    "        df.with_columns(\n",
    "            (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
    "            (\n",
    "                pl.col(\"M11\")\n",
    "                / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3)\n",
    "            ).alias(\"U2\"),\n",
    "        )\n",
    "        .select([\"date_id\", \"target\"] + vars_to_keep)\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n",
    "                for col in vars_to_keep\n",
    "            ]\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "def join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Menggabungkan train dan test berdasarkan kolom yang sama\n",
    "    (untuk memastikan feature engineering konsisten).\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): DataFrame training mentah.\n",
    "        test (pl.DataFrame): DataFrame test mentah.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame hasil concatenation vertical train+test\n",
    "                      pada kolom-kolom yang sama.\n",
    "    \"\"\"\n",
    "    common_columns: list[str] = [\n",
    "        col for col in train.columns if col in test.columns\n",
    "    ]\n",
    "\n",
    "    if \"date_id\" not in common_columns:\n",
    "        raise KeyError(\"'date_id' harus ada di kedua DataFrame.\")\n",
    "\n",
    "    return (\n",
    "        pl.concat(\n",
    "            [train.select(common_columns), test.select(common_columns)],\n",
    "            how=\"vertical\",\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput:\n",
    "    \"\"\"\n",
    "    Memisahkan data menjadi fitur (X) dan target (y), lalu melakukan scaling.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): DataFrame training yang sudah diproses.\n",
    "        test (pl.DataFrame): DataFrame test yang sudah diproses.\n",
    "        features (list[str]): Daftar nama fitur yang digunakan model.\n",
    "\n",
    "    Returns:\n",
    "        DatasetOutput: Dataclass berisi X_train, y_train, X_test, y_test,\n",
    "                       scaler yang sudah di-fit, dan feature_names.\n",
    "    \"\"\"\n",
    "    # Pastikan kolom wajib ada\n",
    "    for col in [\"date_id\", \"target\"]:\n",
    "        if col not in train.columns or col not in test.columns:\n",
    "            raise KeyError(f\"Kolom wajib '{col}' hilang di train/test.\")\n",
    "\n",
    "    X_train = train.drop([\"date_id\", \"target\"])\n",
    "    y_train = train.get_column(\"target\")\n",
    "\n",
    "    X_test = test.drop([\"date_id\", \"target\"])\n",
    "    y_test = test.get_column(\"target\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # fit_transform pada train\n",
    "    X_train_scaled_np = scaler.fit_transform(X_train.to_pandas())\n",
    "    X_train_scaled = pl.from_numpy(X_train_scaled_np, schema=features)\n",
    "\n",
    "    # transform pada test\n",
    "    X_test_scaled_np = scaler.transform(X_test.to_pandas())\n",
    "    X_test_scaled = pl.from_numpy(X_test_scaled_np, schema=features)\n",
    "\n",
    "    return DatasetOutput(\n",
    "        X_train=X_train_scaled,\n",
    "        X_test=X_test_scaled,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        feature_names=features,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa40a2e9",
   "metadata": {
    "papermill": {
     "duration": 0.004673,
     "end_time": "2025-12-01T16:00:13.017703",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.013030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Converting Return Prediction to Signal\n",
    "\n",
    "Here is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cdd7216",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:13.028564Z",
     "iopub.status.busy": "2025-12-01T16:00:13.028187Z",
     "iopub.status.idle": "2025-12-01T16:00:13.034653Z",
     "shell.execute_reply": "2025-12-01T16:00:13.033796Z"
    },
    "papermill": {
     "duration": 0.013959,
     "end_time": "2025-12-01T16:00:13.036351",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.022392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. RETURN -> SIGNAL MAPPING\n",
    "# ============================================================\n",
    "\n",
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray,\n",
    "    params: RetToSignalParameters\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert raw model predictions (expected excess returns) into a trading signal.\n",
    "\n",
    "    Mapping dasar:\n",
    "        signal = clip( ret * signal_multiplier + 1, min_signal, max_signal )\n",
    "\n",
    "    Di mana:\n",
    "        - signal ≈ 1  : posisi netral / benchmark\n",
    "        - signal < 1  : underweight (kurang dari pasar)\n",
    "        - signal > 1  : overweight (lebih agresif dari pasar)\n",
    "\n",
    "    Args:\n",
    "        ret_arr (np.ndarray or scalar-like):\n",
    "            Predicted returns (bisa scalar atau array).\n",
    "        params (RetToSignalParameters):\n",
    "            Parameter scaling dan clipping (min/max signal, multiplier).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            Array sinyal trading dengan shape yang sama seperti input,\n",
    "            sudah di-clip di [min_signal, max_signal].\n",
    "    \"\"\"\n",
    "    # Pastikan dalam bentuk numpy array float\n",
    "    ret_arr = np.asarray(ret_arr, dtype=float)\n",
    "\n",
    "    # Sanity-check: tidak boleh ada NaN / inf\n",
    "    if not np.all(np.isfinite(ret_arr)):\n",
    "        raise ValueError(\n",
    "            \"ret_arr mengandung nilai non-finite (NaN/inf). \"\n",
    "            \"Pastikan prediksi model sudah dibersihkan dulu.\"\n",
    "        )\n",
    "\n",
    "    # Mapping linear dari return -> posisi\n",
    "    raw_signal = ret_arr * params.signal_multiplier + 1.0\n",
    "\n",
    "    # Clip supaya tidak keluar dari range yang diizinkan\n",
    "    signal = np.clip(raw_signal, params.min_signal, params.max_signal)\n",
    "\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10484f",
   "metadata": {
    "papermill": {
     "duration": 0.004513,
     "end_time": "2025-12-01T16:00:13.045810",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.041297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Looking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e52c597",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:13.057364Z",
     "iopub.status.busy": "2025-12-01T16:00:13.056920Z",
     "iopub.status.idle": "2025-12-01T16:00:13.422903Z",
     "shell.execute_reply": "2025-12-01T16:00:13.421871Z"
    },
    "papermill": {
     "duration": 0.37393,
     "end_time": "2025-12-01T16:00:13.424492",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.050562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE ===\n",
      "Train shape: (9011, 98)\n",
      "Test shape : (10, 99)\n",
      "\n",
      "=== DATE RANGE ===\n",
      "Train date_id range: 0 → 9010\n",
      "Test  date_id range: 8980 → 8989\n",
      "\n",
      "=== TRAIN SAMPLE (tail 3) ===\n",
      "shape: (3, 98)\n",
      "┌─────────┬─────┬─────┬─────┬───┬───────────┬─────────────────┬────────────────┬───────────┐\n",
      "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ V9        ┆ forward_returns ┆ risk_free_rate ┆ target    │\n",
      "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---       ┆ ---             ┆ ---            ┆ ---       │\n",
      "│ i32     ┆ f64 ┆ f64 ┆ f64 ┆   ┆ f64       ┆ f64             ┆ f64            ┆ f64       │\n",
      "╞═════════╪═════╪═════╪═════╪═══╪═══════════╪═════════════════╪════════════════╪═══════════╡\n",
      "│ 9008    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.530228 ┆ -0.002897       ┆ 0.0001525      ┆ -0.003362 │\n",
      "│ 9009    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.512769 ┆ -0.027028       ┆ 0.000153       ┆ -0.027493 │\n",
      "│ 9010    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.015503 ┆ 0.015344        ┆ 0.000153       ┆ 0.014879  │\n",
      "└─────────┴─────┴─────┴─────┴───┴───────────┴─────────────────┴────────────────┴───────────┘\n",
      "\n",
      "=== TEST SAMPLE (head 5) ===\n",
      "shape: (5, 3)\n",
      "┌─────────┬───────────┬───────────┐\n",
      "│ date_id ┆ is_scored ┆ target    │\n",
      "│ ---     ┆ ---       ┆ ---       │\n",
      "│ i32     ┆ f64       ┆ f64       │\n",
      "╞═════════╪═══════════╪═══════════╡\n",
      "│ 8980    ┆ 1.0       ┆ 0.003541  │\n",
      "│ 8981    ┆ 1.0       ┆ -0.005964 │\n",
      "│ 8982    ┆ 1.0       ┆ -0.00741  │\n",
      "│ 8983    ┆ 1.0       ┆ 0.00542   │\n",
      "│ 8984    ┆ 1.0       ┆ 0.008357  │\n",
      "└─────────┴───────────┴───────────┘\n",
      "\n",
      "=== TARGET STATS (train.target) ===\n",
      "shape: (9, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ statistic  ┆ target    │\n",
      "│ ---        ┆ ---       │\n",
      "│ str        ┆ f64       │\n",
      "╞════════════╪═══════════╡\n",
      "│ count      ┆ 9011.0    │\n",
      "│ null_count ┆ 0.0       │\n",
      "│ mean       ┆ 0.00005   │\n",
      "│ std        ┆ 0.010562  │\n",
      "│ min        ┆ -0.040582 │\n",
      "│ 25%        ┆ -0.004747 │\n",
      "│ 50%        ┆ 0.000253  │\n",
      "│ 75%        ┆ 0.005479  │\n",
      "│ max        ┆ 0.040551  │\n",
      "└────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. QUICK DATA CHECK: TRAIN & TEST\n",
    "# ============================================================\n",
    "\n",
    "# Load data mentah (sudah kita wrap di helper)\n",
    "train: pl.DataFrame = load_trainset()\n",
    "test: pl.DataFrame  = load_testset()\n",
    "\n",
    "print(\"=== SHAPE ===\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape :\", test.shape)\n",
    "\n",
    "# Range date_id untuk memastikan urut dan tidak kosong\n",
    "print(\"\\n=== DATE RANGE ===\")\n",
    "print(\n",
    "    \"Train date_id range:\",\n",
    "    int(train[\"date_id\"].min()),\n",
    "    \"→\",\n",
    "    int(train[\"date_id\"].max()),\n",
    ")\n",
    "print(\n",
    "    \"Test  date_id range:\",\n",
    "    int(test[\"date_id\"].min()),\n",
    "    \"→\",\n",
    "    int(test[\"date_id\"].max()),\n",
    ")\n",
    "\n",
    "# Cek beberapa baris terakhir train\n",
    "print(\"\\n=== TRAIN SAMPLE (tail 3) ===\")\n",
    "print(train.tail(3))\n",
    "\n",
    "# Cek beberapa kolom penting di test (date_id, is_scored, target)\n",
    "cols_to_show_test = [c for c in [\"date_id\", \"is_scored\", \"target\"] if c in test.columns]\n",
    "print(\"\\n=== TEST SAMPLE (head 5) ===\")\n",
    "print(test.select(cols_to_show_test).head(5))\n",
    "\n",
    "# Statistik dasar target di train\n",
    "print(\"\\n=== TARGET STATS (train.target) ===\")\n",
    "print(train.select(\"target\").describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ab2443",
   "metadata": {
    "papermill": {
     "duration": 0.004574,
     "end_time": "2025-12-01T16:00:13.434309",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.429735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating the Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29061070",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:13.445981Z",
     "iopub.status.busy": "2025-12-01T16:00:13.444973Z",
     "iopub.status.idle": "2025-12-01T16:00:13.553762Z",
     "shell.execute_reply": "2025-12-01T16:00:13.552697Z"
    },
    "papermill": {
     "duration": 0.116237,
     "end_time": "2025-12-01T16:00:13.555356",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.439119",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7510, 13)\n",
      "X_test  shape: (20, 13)\n",
      "y_train length: 7510\n",
      "y_test  length: 20\n",
      "Num features: 13\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. FEATURE ENGINEERING + TRAIN/TEST SPLIT\n",
    "# ============================================================\n",
    "\n",
    "# 1) Gabungkan train & test mentah pada kolom yang sama (supaya FE konsisten)\n",
    "df_all: pl.DataFrame = join_train_test_dataframes(train, test)\n",
    "\n",
    "# 2) Terapkan feature engineering baseline (U1, U2, subset fitur, EWM fill, drop_null)\n",
    "df_fe: pl.DataFrame = create_example_dataset(df=df_all)\n",
    "\n",
    "# 3) Kembalikan lagi ke train_fe dan test_fe berdasarkan date_id awal\n",
    "train_ids = train.get_column(\"date_id\").unique()\n",
    "test_ids  = test.get_column(\"date_id\").unique()\n",
    "\n",
    "train_fe: pl.DataFrame = df_fe.filter(pl.col(\"date_id\").is_in(train_ids))\n",
    "test_fe: pl.DataFrame  = df_fe.filter(pl.col(\"date_id\").is_in(test_ids))\n",
    "\n",
    "# 4) Definisikan daftar fitur (semua kolom kecuali 'date_id' dan 'target')\n",
    "FEATURES: list[str] = [\n",
    "    col for col in test_fe.columns if col not in [\"date_id\", \"target\"]\n",
    "]\n",
    "\n",
    "# 5) Split menjadi X/y + scaling, dibungkus dalam DatasetOutput\n",
    "dataset: DatasetOutput = split_dataset(\n",
    "    train=train_fe,\n",
    "    test=test_fe,\n",
    "    features=FEATURES,\n",
    ")\n",
    "\n",
    "X_train: pl.DataFrame = dataset.X_train\n",
    "X_test: pl.DataFrame  = dataset.X_test\n",
    "y_train: pl.Series    = dataset.y_train\n",
    "y_test: pl.Series     = dataset.y_test\n",
    "scaler: StandardScaler = dataset.scaler\n",
    "feature_names: list[str] | None = dataset.feature_names\n",
    "\n",
    "# (Opsional) Quick check\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test  shape:\", X_test.shape)\n",
    "print(\"y_train length:\", y_train.len())\n",
    "print(\"y_test  length:\", y_test.len())\n",
    "print(\"Num features:\", len(FEATURES))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2b9df",
   "metadata": {
    "papermill": {
     "duration": 0.005732,
     "end_time": "2025-12-01T16:00:13.566455",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.560723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fitting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40025530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:13.577642Z",
     "iopub.status.busy": "2025-12-01T16:00:13.577297Z",
     "iopub.status.idle": "2025-12-01T16:00:13.881070Z",
     "shell.execute_reply": "2025-12-01T16:00:13.880144Z"
    },
    "papermill": {
     "duration": 0.311495,
     "end_time": "2025-12-01T16:00:13.882901",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.571406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ElasticNet Baseline Fitted ===\n",
      "Best alpha (CV) : 0.000266\n",
      "L1 ratio        : 0.5\n",
      "CV folds        : 10\n",
      "Train R²        : 0.004032\n",
      "Train MSE       : 1.225545e-04\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. FIT ELASTICNET BASELINE (WITH CV)\n",
    "# ============================================================\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Sklearn lebih aman kalau dikasih numpy array\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "# Siapkan argumen untuk ElasticNetCV dari enet_params + tambahan\n",
    "enet_cv_kwargs = asdict(enet_params).copy()\n",
    "enet_cv_kwargs.update(\n",
    "    {\n",
    "        \"fit_intercept\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"n_jobs\": -1,   # pakai semua core yang tersedia\n",
    "    }\n",
    ")\n",
    "\n",
    "# 1) Cross-validated ElasticNet untuk cari alpha terbaik\n",
    "model_cv: ElasticNetCV = ElasticNetCV(**enet_cv_kwargs)\n",
    "model_cv.fit(X_train_np, y_train_np)\n",
    "\n",
    "best_alpha: float = float(model_cv.alpha_)\n",
    "\n",
    "# 2) Fit final ElasticNet dengan alpha terbaik\n",
    "model: ElasticNet = ElasticNet(\n",
    "    alpha=best_alpha,\n",
    "    l1_ratio=enet_params.l1_ratio,\n",
    "    max_iter=enet_params.max_iter,\n",
    "    fit_intercept=True,\n",
    "    random_state=SEED,\n",
    ")\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Quick sanity check: performa di training\n",
    "# ------------------------------------------------------------\n",
    "y_pred_train = model.predict(X_train_np)\n",
    "r2_train = r2_score(y_train_np, y_pred_train)\n",
    "mse_train = mean_squared_error(y_train_np, y_pred_train)\n",
    "\n",
    "print(\"=== ElasticNet Baseline Fitted ===\")\n",
    "print(f\"Best alpha (CV) : {best_alpha:.6f}\")\n",
    "print(f\"L1 ratio        : {enet_params.l1_ratio}\")\n",
    "print(f\"CV folds        : {enet_params.cv}\")\n",
    "print(f\"Train R²        : {r2_train:.6f}\")\n",
    "print(f\"Train MSE       : {mse_train:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55036132",
   "metadata": {
    "papermill": {
     "duration": 0.005726,
     "end_time": "2025-12-01T16:00:13.893753",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.888027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function via Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ed5197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:13.905865Z",
     "iopub.status.busy": "2025-12-01T16:00:13.905490Z",
     "iopub.status.idle": "2025-12-01T16:00:13.913133Z",
     "shell.execute_reply": "2025-12-01T16:00:13.912261Z"
    },
    "papermill": {
     "duration": 0.0156,
     "end_time": "2025-12-01T16:00:13.914839",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.899239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. PREDICTION FUNCTION FOR KAGGLE EVALUATION API\n",
    "# ============================================================\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Fungsi prediksi yang dipanggil oleh Kaggle Evaluation API.\n",
    "\n",
    "    Alur:\n",
    "    1) Rename kolom target (lagged_forward_returns -> target) jika perlu.\n",
    "    2) Terapkan feature engineering (create_example_dataset).\n",
    "    3) Pilih fitur sesuai FEATURES dan lakukan scaling dengan scaler yang sudah di-fit.\n",
    "    4) Prediksi expected excess return pakai model.\n",
    "    5) Konversi return -> sinyal trading via convert_ret_to_signal.\n",
    "    6) Return satu nilai float (sinyal) untuk batch pertama.\n",
    "\n",
    "    Catatan:\n",
    "    - DefaultInferenceServer biasanya memanggil fungsi ini dengan\n",
    "      1 baris per call, tapi kita tetap handle jika >1 baris.\n",
    "    \"\"\"\n",
    "    # 1. Pastikan kolom target ada\n",
    "    if \"target\" not in test.columns:\n",
    "        if \"lagged_forward_returns\" in test.columns:\n",
    "            test = test.rename({\"lagged_forward_returns\": \"target\"})\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Test dataframe harus memiliki kolom 'target' atau 'lagged_forward_returns'.\"\n",
    "            )\n",
    "\n",
    "    # 2. Pastikan 'date_id' ada\n",
    "    if \"date_id\" not in test.columns:\n",
    "        raise KeyError(\"Kolom 'date_id' wajib ada di dataframe test.\")\n",
    "\n",
    "    # 3. Feature engineering (U1, U2, imputasi EWM, subset fitur, drop_null)\n",
    "    df = create_example_dataset(test)\n",
    "\n",
    "    # Jika setelah FE tidak ada baris (misal semua null dan ter-drop),\n",
    "    # kembalikan sinyal netral 1.0 agar tidak crash saat evaluasi.\n",
    "    if df.height == 0:\n",
    "        return float(1.0)\n",
    "\n",
    "    # 4. Ambil hanya fitur yang sudah didefinisikan di training\n",
    "    X_test = df.select(FEATURES)\n",
    "\n",
    "    # Sanity-check: pastikan semua fitur ada\n",
    "    missing_feats = [f for f in FEATURES if f not in X_test.columns]\n",
    "    if missing_feats:\n",
    "        raise KeyError(\n",
    "            f\"Fitur berikut hilang di data FE test: {missing_feats}\"\n",
    "        )\n",
    "\n",
    "    # 5. Sklearn pakai numpy array\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    X_test_scaled_np = scaler.transform(X_test_np)\n",
    "\n",
    "    # 6. Prediksi expected excess return\n",
    "    raw_pred = model.predict(X_test_scaled_np)  # array shape (n_samples,)\n",
    "\n",
    "    # 7. Konversi ke sinyal trading\n",
    "    signal_arr = convert_ret_to_signal(raw_pred, ret_signal_params)\n",
    "\n",
    "    # 8. Ambil satu nilai (biasanya satu baris per call)\n",
    "    return float(signal_arr[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76802b15",
   "metadata": {
    "papermill": {
     "duration": 0.004749,
     "end_time": "2025-12-01T16:00:13.924942",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.920193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Launch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48332816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:00:13.937361Z",
     "iopub.status.busy": "2025-12-01T16:00:13.937031Z",
     "iopub.status.idle": "2025-12-01T16:00:14.193138Z",
     "shell.execute_reply": "2025-12-01T16:00:14.191803Z"
    },
    "papermill": {
     "duration": 0.26491,
     "end_time": "2025-12-01T16:00:14.194924",
     "exception": false,
     "start_time": "2025-12-01T16:00:13.930014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in LOCAL GATEWAY mode for debugging...\n",
      "Using input dir: /kaggle/input/hull-tactical-market-prediction\n",
      "\n",
      "submission.parquet generated at: /kaggle/working/submission.parquet\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. START KAGGLE EVALUATION SERVER\n",
    "# ============================================================\n",
    "\n",
    "inference_server = kei.DefaultInferenceServer(predict)\n",
    "\n",
    "if IS_COMP_RERUN:\n",
    "    # Mode ini dipakai saat SUBMIT notebook ke kompetisi\n",
    "    print(\"Detected competition rerun environment. Starting inference server...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Mode lokal / saat run manual di Notebook\n",
    "    print(\"Running in LOCAL GATEWAY mode for debugging...\")\n",
    "    print(f\"Using input dir: {INPUT_DIR}\")\n",
    "    inference_server.run_local_gateway((str(INPUT_DIR),))\n",
    "\n",
    "    # Setelah lokal gateway selesai, cek apakah submission.parquet sudah dibuat\n",
    "    sub_path = Path(\"submission.parquet\")\n",
    "    if sub_path.exists():\n",
    "        print(\"\\nsubmission.parquet generated at:\", sub_path.resolve())\n",
    "    else:\n",
    "        print(\"\\nWARNING: submission.parquet not found in working directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.806648,
   "end_time": "2025-12-01T16:00:15.022591",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T16:00:02.215943",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
