{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a78a06b",
   "metadata": {
    "papermill": {
     "duration": 0.006266,
     "end_time": "2025-12-01T16:42:40.981002",
     "exception": false,
     "start_time": "2025-12-01T16:42:40.974736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cd593c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:40.992708Z",
     "iopub.status.busy": "2025-12-01T16:42:40.992339Z",
     "iopub.status.idle": "2025-12-01T16:42:46.050391Z",
     "shell.execute_reply": "2025-12-01T16:42:46.049490Z"
    },
    "papermill": {
     "duration": 5.065656,
     "end_time": "2025-12-01T16:42:46.052146",
     "exception": false,
     "start_time": "2025-12-01T16:42:40.986490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Imports & Global Setup\n",
    "# ============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.auto import tqdm  # lebih aman di notebook\n",
    "\n",
    "import kaggle_evaluation.default_inference_server as kei\n",
    "# nantinya dipakai sebagai: kei.DefaultInferenceServer(...)\n",
    "\n",
    "# ============================================================\n",
    "# 2. Reproducibility\n",
    "# ============================================================\n",
    "SEED: int = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# optional: kalau nanti pakai model lain yang ada random_state, pakai SEED ini\n",
    "\n",
    "# ============================================================\n",
    "# 3. Warning / Logging Setup (minimal)\n",
    "# ============================================================\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbc9757",
   "metadata": {
    "papermill": {
     "duration": 0.004593,
     "end_time": "2025-12-01T16:42:46.062179",
     "exception": false,
     "start_time": "2025-12-01T16:42:46.057586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d6e1429",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:46.073723Z",
     "iopub.status.busy": "2025-12-01T16:42:46.073222Z",
     "iopub.status.idle": "2025-12-01T16:42:46.082784Z",
     "shell.execute_reply": "2025-12-01T16:42:46.081594Z"
    },
    "papermill": {
     "duration": 0.017516,
     "end_time": "2025-12-01T16:42:46.084459",
     "exception": false,
     "start_time": "2025-12-01T16:42:46.066943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hull-tactical-market-prediction/train.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/test.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ae5c6",
   "metadata": {
    "papermill": {
     "duration": 0.004576,
     "end_time": "2025-12-01T16:42:46.094080",
     "exception": false,
     "start_time": "2025-12-01T16:42:46.089504",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04b1280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:46.105709Z",
     "iopub.status.busy": "2025-12-01T16:42:46.105363Z",
     "iopub.status.idle": "2025-12-01T16:42:52.080503Z",
     "shell.execute_reply": "2025-12-01T16:42:52.079539Z"
    },
    "papermill": {
     "duration": 5.983344,
     "end_time": "2025-12-01T16:42:52.082285",
     "exception": false,
     "start_time": "2025-12-01T16:42:46.098941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & GLOBAL SETUP\n",
    "# ============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Optional: tree-based model (akan dipakai nanti untuk naikkan skor)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except ImportError:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "import kaggle_evaluation.default_inference_server as kei\n",
    "\n",
    "# Reproducibility\n",
    "SEED: int = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# Sedikit bersihin warning yang kurang penting\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. PROJECT DIRECTORY STRUCTURE & CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Nama kompetisi (dipakai untuk bikin folder kerja terpisah)\n",
    "COMP_NAME: str = \"hull-tactical-market-prediction\"\n",
    "\n",
    "# Nama eksperimen (bisa kamu ganti saat coba konfigurasi lain)\n",
    "EXPERIMENT_NAME: str = f\"enet_lgbm_v1_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# ---- Input & Working Dirs ----\n",
    "INPUT_DIR: Path = Path(\"/kaggle/input\") / COMP_NAME\n",
    "WORK_DIR: Path  = Path(\"/kaggle/working\") / COMP_NAME\n",
    "\n",
    "# Pastikan WORK_DIR ada (supaya semua output ngumpul di sini)\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File data utama\n",
    "TRAIN_PATH: Path = INPUT_DIR / \"train.csv\"\n",
    "TEST_PATH: Path  = INPUT_DIR / \"test.csv\"\n",
    "\n",
    "# Folder resmi dari Kaggle Evaluation API (source & copy ke working jika perlu)\n",
    "KAGGLE_EVAL_SRC: Path  = INPUT_DIR / \"kaggle_evaluation\"\n",
    "KAGGLE_EVAL_WORK: Path = WORK_DIR / \"kaggle_evaluation\"\n",
    "KAGGLE_EVAL_WORK.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Output structure (untuk hasil eksperimen) ----\n",
    "OUT_DIR: Path        = WORK_DIR / \"outputs\"\n",
    "MODEL_DIR: Path      = OUT_DIR / \"models\"       # simpan model, scaler, dsb.\n",
    "FEATURE_DIR: Path    = OUT_DIR / \"features\"     # simpan dataset hasil FE (opsional)\n",
    "LOG_DIR: Path        = OUT_DIR / \"logs\"         # catatan eksperimen, metrik\n",
    "SUBMISSION_DIR: Path = OUT_DIR / \"submissions\"  # submission lokal untuk dicek\n",
    "\n",
    "for p in [OUT_DIR, MODEL_DIR, FEATURE_DIR, LOG_DIR, SUBMISSION_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Flag environment: apakah ini run dalam mode kompetisi (rerun) atau lokal\n",
    "IS_COMP_RERUN: bool = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") is not None\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. RETURNS -> SIGNAL CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SignalConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    min_signal / max_signal : batas posisi (0 = cash, 2 = 2x leverage)\n",
    "    multiplier              : skala sensitivitas sinyal terhadap prediksi return\n",
    "    \"\"\"\n",
    "    min_signal: float = 0.0\n",
    "    max_signal: float = 2.0\n",
    "    multiplier: float = 400.0\n",
    "\n",
    "\n",
    "# Beberapa preset sinyal yang nanti bisa kamu coba via validasi Sharpe\n",
    "SIGNAL_PRESETS: Dict[str, SignalConfig] = {\n",
    "    \"baseline\": SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=400.0),\n",
    "    \"conservative\": SignalConfig(min_signal=0.5, max_signal=1.5, multiplier=200.0),\n",
    "    \"aggressive\": SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=600.0),\n",
    "}\n",
    "\n",
    "ACTIVE_SIGNAL_KEY: str = \"baseline\"  # ganti di sini kalau mau coba preset lain\n",
    "SIGNAL_CFG: SignalConfig = SIGNAL_PRESETS[ACTIVE_SIGNAL_KEY]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. MODEL & EVALUATION CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi utama untuk model ElasticNet baseline.\n",
    "    Akan jadi 'core signal' yang stabil dan cepat.\n",
    "    \"\"\"\n",
    "    cv_folds: int = 10\n",
    "    l1_ratio: float = 0.5\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: np.logspace(-4, 2, 100)\n",
    "    )\n",
    "    max_iter: int = 1_000_000\n",
    "    random_state: int = SEED\n",
    "\n",
    "ENET_CFG = ModelConfig()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GBMConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi default untuk model tree-based (LightGBM).\n",
    "\n",
    "    Model ini yang nanti akan kita pakai untuk menangkap non-linearitas\n",
    "    dan interaksi fitur, kemudian di-ensemble dengan ElasticNet.\n",
    "    \"\"\"\n",
    "    num_leaves: int = 31\n",
    "    max_depth: int = -1          # -1 = unlimited, tapi kita nanti bisa kecilkan\n",
    "    learning_rate: float = 0.03\n",
    "    n_estimators: int = 500\n",
    "    subsample: float = 0.8       # row subsampling\n",
    "    colsample_bytree: float = 0.8\n",
    "    reg_alpha: float = 0.0\n",
    "    reg_lambda: float = 1.0\n",
    "    random_state: int = SEED\n",
    "\n",
    "GBM_CFG = GBMConfig()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi evaluasi internal (Sharpe, validasi time-series).\n",
    "    \"\"\"\n",
    "    n_folds: int = 5                 # jumlah fold time-series CV\n",
    "    val_fraction: float = 0.2        # porsi akhir data untuk pure hold-out (opsional)\n",
    "    sharpe_annualization_factor: float = np.sqrt(252.0)  # 252 hari bursa setahun\n",
    "\n",
    "EVAL_CFG = EvaluationConfig()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5097be86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.093904Z",
     "iopub.status.busy": "2025-12-01T16:42:52.093253Z",
     "iopub.status.idle": "2025-12-01T16:42:52.103110Z",
     "shell.execute_reply": "2025-12-01T16:42:52.101992Z"
    },
    "papermill": {
     "duration": 0.017311,
     "end_time": "2025-12-01T16:42:52.104611",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.087300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR : /kaggle/working/hull-tactical-market-prediction\n",
      "TRAIN_PATH exists: True\n",
      "TEST_PATH exists : True\n",
      "Signal config    : SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=400.0)\n",
      "Model config     : ModelConfig(cv_folds=10, l1_ratio=0.5, alphas=array([1.00000000e-04, 1.14975700e-04, 1.32194115e-04, 1.51991108e-04,\n",
      "       1.74752840e-04, 2.00923300e-04, 2.31012970e-04, 2.65608778e-04,\n",
      "       3.05385551e-04, 3.51119173e-04, 4.03701726e-04, 4.64158883e-04,\n",
      "       5.33669923e-04, 6.13590727e-04, 7.05480231e-04, 8.11130831e-04,\n",
      "       9.32603347e-04, 1.07226722e-03, 1.23284674e-03, 1.41747416e-03,\n",
      "       1.62975083e-03, 1.87381742e-03, 2.15443469e-03, 2.47707636e-03,\n",
      "       2.84803587e-03, 3.27454916e-03, 3.76493581e-03, 4.32876128e-03,\n",
      "       4.97702356e-03, 5.72236766e-03, 6.57933225e-03, 7.56463328e-03,\n",
      "       8.69749003e-03, 1.00000000e-02, 1.14975700e-02, 1.32194115e-02,\n",
      "       1.51991108e-02, 1.74752840e-02, 2.00923300e-02, 2.31012970e-02,\n",
      "       2.65608778e-02, 3.05385551e-02, 3.51119173e-02, 4.03701726e-02,\n",
      "       4.64158883e-02, 5.33669923e-02, 6.13590727e-02, 7.05480231e-02,\n",
      "       8.11130831e-02, 9.32603347e-02, 1.07226722e-01, 1.23284674e-01,\n",
      "       1.41747416e-01, 1.62975083e-01, 1.87381742e-01, 2.15443469e-01,\n",
      "       2.47707636e-01, 2.84803587e-01, 3.27454916e-01, 3.76493581e-01,\n",
      "       4.32876128e-01, 4.97702356e-01, 5.72236766e-01, 6.57933225e-01,\n",
      "       7.56463328e-01, 8.69749003e-01, 1.00000000e+00, 1.14975700e+00,\n",
      "       1.32194115e+00, 1.51991108e+00, 1.74752840e+00, 2.00923300e+00,\n",
      "       2.31012970e+00, 2.65608778e+00, 3.05385551e+00, 3.51119173e+00,\n",
      "       4.03701726e+00, 4.64158883e+00, 5.33669923e+00, 6.13590727e+00,\n",
      "       7.05480231e+00, 8.11130831e+00, 9.32603347e+00, 1.07226722e+01,\n",
      "       1.23284674e+01, 1.41747416e+01, 1.62975083e+01, 1.87381742e+01,\n",
      "       2.15443469e+01, 2.47707636e+01, 2.84803587e+01, 3.27454916e+01,\n",
      "       3.76493581e+01, 4.32876128e+01, 4.97702356e+01, 5.72236766e+01,\n",
      "       6.57933225e+01, 7.56463328e+01, 8.69749003e+01, 1.00000000e+02]), max_iter=1000000, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(\"WORK_DIR :\", WORK_DIR)\n",
    "print(\"TRAIN_PATH exists:\", TRAIN_PATH.exists())\n",
    "print(\"TEST_PATH exists :\", TEST_PATH.exists())\n",
    "print(\"Signal config    :\", SIGNAL_CFG)\n",
    "print(\"Model config     :\", ENET_CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2734f61",
   "metadata": {
    "papermill": {
     "duration": 0.004563,
     "end_time": "2025-12-01T16:42:52.114152",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.109589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataclasses Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7f6c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.126248Z",
     "iopub.status.busy": "2025-12-01T16:42:52.125863Z",
     "iopub.status.idle": "2025-12-01T16:42:52.136588Z",
     "shell.execute_reply": "2025-12-01T16:42:52.135542Z"
    },
    "papermill": {
     "duration": 0.018098,
     "end_time": "2025-12-01T16:42:52.138089",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.119991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. DATACLASSES HELPERS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    \"\"\"\n",
    "    Paket hasil preprocessing dataset.\n",
    "\n",
    "    - X_train, y_train : data untuk melatih model\n",
    "    - X_test,  y_test  : data untuk evaluasi (mock test / hold-out)\n",
    "    - scaler           : StandardScaler yang sudah di-fit pada X_train\n",
    "    - feature_names    : daftar nama fitur (opsional, tapi berguna untuk debugging)\n",
    "    \"\"\"\n",
    "    X_train: pl.DataFrame\n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "    feature_names: list[str] | None = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ElasticNetParameters:\n",
    "    \"\"\"\n",
    "    Parameter yang dipakai ketika membangun ElasticNetCV / ElasticNet.\n",
    "\n",
    "    Default-nya diambil dari ENET_CFG (ModelConfig) supaya konsisten\n",
    "    dengan konfigurasi global, tapi tetap bisa dioverride kalau perlu.\n",
    "    \"\"\"\n",
    "    l1_ratio: float = ENET_CFG.l1_ratio\n",
    "    cv: int = ENET_CFG.cv_folds\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: ENET_CFG.alphas.copy()\n",
    "    )\n",
    "    max_iter: int = ENET_CFG.max_iter\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not (0.0 <= self.l1_ratio <= 1.0):\n",
    "            raise ValueError(\n",
    "                \"ElasticNet l1_ratio harus berada di dalam interval [0, 1].\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    Default diambil dari SIGNAL_CFG supaya selaras dengan konfigurasi global.\n",
    "    \"\"\"\n",
    "    signal_multiplier: float = SIGNAL_CFG.multiplier\n",
    "    min_signal: float = SIGNAL_CFG.min_signal\n",
    "    max_signal: float = SIGNAL_CFG.max_signal\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.min_signal >= self.max_signal:\n",
    "            raise ValueError(\n",
    "                \"min_signal harus lebih kecil daripada max_signal.\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8650660",
   "metadata": {
    "papermill": {
     "duration": 0.00461,
     "end_time": "2025-12-01T16:42:52.147969",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.143359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4347418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.159272Z",
     "iopub.status.busy": "2025-12-01T16:42:52.158879Z",
     "iopub.status.idle": "2025-12-01T16:42:52.173047Z",
     "shell.execute_reply": "2025-12-01T16:42:52.172095Z"
    },
    "papermill": {
     "duration": 0.021812,
     "end_time": "2025-12-01T16:42:52.174608",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.152796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. DATACLASSES HELPERS + PARAMETER OBJECTS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    \"\"\"\n",
    "    Paket hasil preprocessing dataset.\n",
    "\n",
    "    - X_train, y_train : data untuk melatih model\n",
    "    - X_test,  y_test  : data untuk evaluasi (mock test / hold-out)\n",
    "    - scaler           : StandardScaler yang sudah di-fit pada X_train\n",
    "    - feature_names    : daftar nama fitur (opsional, untuk debugging/analisis)\n",
    "    - dates_train      : date_id untuk baris X_train (opsional, untuk TS-CV)\n",
    "    - dates_test       : date_id untuk baris X_test  (opsional, untuk TS-CV)\n",
    "    \"\"\"\n",
    "    X_train: pl.DataFrame\n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "    feature_names: list[str] | None = None\n",
    "    dates_train: pl.Series | None = None\n",
    "    dates_test: pl.Series | None = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ElasticNetParameters:\n",
    "    \"\"\"\n",
    "    Parameter yang dipakai ketika membangun ElasticNetCV / ElasticNet.\n",
    "\n",
    "    Default diambil dari ENET_CFG (ModelConfig) supaya konsisten dengan\n",
    "    konfigurasi global, tapi bisa dioverride kalau perlu.\n",
    "    \"\"\"\n",
    "    l1_ratio: float = ENET_CFG.l1_ratio\n",
    "    cv: int = ENET_CFG.cv_folds\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: ENET_CFG.alphas.copy()\n",
    "    )\n",
    "    max_iter: int = ENET_CFG.max_iter\n",
    "    random_state: int = ENET_CFG.random_state\n",
    "    n_jobs: int = -1\n",
    "    fit_intercept: bool = True\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not (0.0 <= self.l1_ratio <= 1.0):\n",
    "            raise ValueError(\n",
    "                \"ElasticNet l1_ratio harus berada di dalam interval [0, 1].\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    Default diambil dari SIGNAL_CFG supaya selaras dengan konfigurasi global.\n",
    "    \"\"\"\n",
    "    signal_multiplier: float = SIGNAL_CFG.multiplier\n",
    "    min_signal: float = SIGNAL_CFG.min_signal\n",
    "    max_signal: float = SIGNAL_CFG.max_signal\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.min_signal >= self.max_signal:\n",
    "            raise ValueError(\n",
    "                \"min_signal harus lebih kecil daripada max_signal.\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class GBMParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk model tree-based (LightGBM) yang akan kita gunakan\n",
    "    sebagai pelengkap model linear (ElasticNet/Ridge).\n",
    "\n",
    "    Default diambil dari GBM_CFG (GBMConfig).\n",
    "    \"\"\"\n",
    "    num_leaves: int = GBM_CFG.num_leaves\n",
    "    max_depth: int = GBM_CFG.max_depth\n",
    "    learning_rate: float = GBM_CFG.learning_rate\n",
    "    n_estimators: int = GBM_CFG.n_estimators\n",
    "    subsample: float = GBM_CFG.subsample\n",
    "    colsample_bytree: float = GBM_CFG.colsample_bytree\n",
    "    reg_alpha: float = GBM_CFG.reg_alpha\n",
    "    reg_lambda: float = GBM_CFG.reg_lambda\n",
    "    random_state: int = GBM_CFG.random_state\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Instansiasi objek parameter yang akan dipakai di pipeline\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "ret_signal_params = RetToSignalParameters()\n",
    "enet_params       = ElasticNetParameters()\n",
    "gbm_params: Optional[GBMParameters] = GBMParameters() if HAS_LGBM else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3befa5e3",
   "metadata": {
    "papermill": {
     "duration": 0.004528,
     "end_time": "2025-12-01T16:42:52.184226",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.179698",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Loading/Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9660c1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.195713Z",
     "iopub.status.busy": "2025-12-01T16:42:52.195359Z",
     "iopub.status.idle": "2025-12-01T16:42:52.214568Z",
     "shell.execute_reply": "2025-12-01T16:42:52.213613Z"
    },
    "papermill": {
     "duration": 0.026741,
     "end_time": "2025-12-01T16:42:52.216216",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.189475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. DATA LOADING & PREPROCESSING HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def load_trainset(path: Path = TRAIN_PATH, drop_last_n: int = 10) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dan praproses training dataset.\n",
    "\n",
    "    - Mengganti nama kolom target menjadi 'target'\n",
    "      (market_forward_excess_returns -> target).\n",
    "    - Meng-cast semua kolom selain 'date_id' ke Float64.\n",
    "    - Sorting berdasarkan date_id.\n",
    "    - Opsional: membuang N baris terakhir (drop_last_n) untuk\n",
    "      menghindari kebocoran saat mock test.\n",
    "\n",
    "    Args:\n",
    "        path (Path): lokasi file train.csv.\n",
    "        drop_last_n (int): jumlah baris terakhir yang dibuang.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame training yang sudah rapi.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pl.read_csv(path)\n",
    "        .rename({\"market_forward_excess_returns\": \"target\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"date_id\").cast(pl.Int32, strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "    if drop_last_n > 0:\n",
    "        df = df.head(-drop_last_n)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_testset(path: Path = TEST_PATH) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dan praproses test/mock dataset.\n",
    "\n",
    "    - Mengganti nama 'lagged_forward_returns' -> 'target' agar\n",
    "      struktur mirip train (meski tidak dipakai sebagai ground truth).\n",
    "    - Meng-cast semua kolom selain 'date_id' ke Float64.\n",
    "    - Sorting berdasarkan date_id.\n",
    "\n",
    "    Args:\n",
    "        path (Path): lokasi file test.csv.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame test yang sudah rapi.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pl.read_csv(path)\n",
    "        .rename({\"lagged_forward_returns\": \"target\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"date_id\").cast(pl.Int32, strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Membuat fitur baseline + beberapa fitur tambahan (momentum & rolling),\n",
    "    lalu membersihkan DataFrame.\n",
    "\n",
    "    Fitur baru:\n",
    "        - U1 = I2 - I1\n",
    "        - U2 = M11 / mean(I2, I9, I7)\n",
    "        - Untuk setiap fitur dasar F di base_vars:\n",
    "            * F_diff1 : F_t - F_{t-1}   (momentum harian)\n",
    "            * F_rm5   : rolling mean 5 hari\n",
    "\n",
    "    Setelah itu:\n",
    "        - Pilih subset kolom:\n",
    "            ['date_id', 'target'] + semua fitur di vars_to_keep\n",
    "        - Imputasi missing dengan exponential weighted mean (EWM)\n",
    "        - Drop baris yang masih mengandung null\n",
    "        - Sort by date_id\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): input Polars DataFrame (train+test gabungan).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame dengan fitur baru & tanpa null.\n",
    "    \"\"\"\n",
    "    # Fitur dasar yang akan dipakai sebagai anchor untuk FE\n",
    "    base_vars: List[str] = [\n",
    "        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "        \"P10\", \"P12\", \"P13\",\n",
    "    ]\n",
    "\n",
    "    # Kolom yang wajib ada untuk menghitung U1 & U2\n",
    "    required_base_cols = [\"I1\", \"I2\", \"M11\", \"I7\", \"I9\"] + base_vars\n",
    "    missing = [c for c in required_base_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Kolom berikut hilang di DataFrame: {missing}\")\n",
    "\n",
    "    # Pastikan urutan berdasarkan waktu\n",
    "    df = df.sort(\"date_id\")\n",
    "\n",
    "    # Fitur U1 & U2\n",
    "    df_feat = df.with_columns(\n",
    "        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
    "        (\n",
    "            pl.col(\"M11\")\n",
    "            / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3.0)\n",
    "        ).alias(\"U2\"),\n",
    "    )\n",
    "\n",
    "    # Buat fitur momentum & rolling mean 5 hari untuk setiap base_var\n",
    "    derived_vars: List[str] = []\n",
    "    for col in base_vars:\n",
    "        diff_name = f\"{col}_diff1\"\n",
    "        rm5_name  = f\"{col}_rm5\"\n",
    "\n",
    "        df_feat = df_feat.with_columns(\n",
    "            (pl.col(col) - pl.col(col).shift(1)).alias(diff_name),\n",
    "            pl.col(col).rolling_mean(window_size=5, min_periods=1).alias(rm5_name),\n",
    "        )\n",
    "\n",
    "        derived_vars.extend([diff_name, rm5_name])\n",
    "\n",
    "    # Kumpulan semua fitur yang akan kita pakai\n",
    "    vars_to_keep: List[str] = base_vars + derived_vars + [\"U1\", \"U2\"]\n",
    "\n",
    "    # Imputasi missing dengan EWM untuk semua fitur\n",
    "    df_feat = (\n",
    "        df_feat\n",
    "        .select([\"date_id\", \"target\"] + vars_to_keep)\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n",
    "                for col in vars_to_keep\n",
    "            ]\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "def join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Menggabungkan train dan test berdasarkan kolom yang sama\n",
    "    (untuk memastikan feature engineering konsisten).\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): DataFrame training mentah.\n",
    "        test (pl.DataFrame): DataFrame test mentah.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame hasil concatenation vertical train+test\n",
    "                      pada kolom-kolom yang sama.\n",
    "    \"\"\"\n",
    "    common_columns: list[str] = [\n",
    "        col for col in train.columns if col in test.columns\n",
    "    ]\n",
    "\n",
    "    if \"date_id\" not in common_columns:\n",
    "        raise KeyError(\"'date_id' harus ada di kedua DataFrame.\")\n",
    "\n",
    "    return (\n",
    "        pl.concat(\n",
    "            [train.select(common_columns), test.select(common_columns)],\n",
    "            how=\"vertical\",\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput:\n",
    "    \"\"\"\n",
    "    Memisahkan data menjadi fitur (X) dan target (y), lalu melakukan scaling.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): DataFrame training yang sudah diproses.\n",
    "        test (pl.DataFrame): DataFrame test yang sudah diproses.\n",
    "        features (list[str]): Daftar nama fitur yang digunakan model.\n",
    "\n",
    "    Returns:\n",
    "        DatasetOutput: Dataclass berisi X_train, y_train, X_test, y_test,\n",
    "                       scaler yang sudah di-fit, feature_names,\n",
    "                       dan date_id masing-masing set.\n",
    "    \"\"\"\n",
    "    # Pastikan kolom wajib ada\n",
    "    for col in [\"date_id\", \"target\"]:\n",
    "        if col not in train.columns or col not in test.columns:\n",
    "            raise KeyError(f\"Kolom wajib '{col}' hilang di train/test.\")\n",
    "\n",
    "    # Simpan date_id untuk keperluan time-series CV nanti\n",
    "    dates_train = train.get_column(\"date_id\")\n",
    "    dates_test  = test.get_column(\"date_id\")\n",
    "\n",
    "    X_train = train.drop([\"date_id\", \"target\"])\n",
    "    y_train = train.get_column(\"target\")\n",
    "\n",
    "    X_test = test.drop([\"date_id\", \"target\"])\n",
    "    y_test = test.get_column(\"target\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Sklearn lebih nyaman kalau dikasih numpy array\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np  = X_test.to_numpy()\n",
    "\n",
    "    # fit_transform pada train\n",
    "    X_train_scaled_np = scaler.fit_transform(X_train_np)\n",
    "    X_train_scaled = pl.from_numpy(X_train_scaled_np, schema=features)\n",
    "\n",
    "    # transform pada test\n",
    "    X_test_scaled_np = scaler.transform(X_test_np)\n",
    "    X_test_scaled = pl.from_numpy(X_test_scaled_np, schema=features)\n",
    "\n",
    "    return DatasetOutput(\n",
    "        X_train=X_train_scaled,\n",
    "        X_test=X_test_scaled,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        feature_names=features,\n",
    "        dates_train=dates_train,\n",
    "        dates_test=dates_test,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672efb0",
   "metadata": {
    "papermill": {
     "duration": 0.004556,
     "end_time": "2025-12-01T16:42:52.225673",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.221117",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Converting Return Prediction to Signal\n",
    "\n",
    "Here is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad2484a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.236661Z",
     "iopub.status.busy": "2025-12-01T16:42:52.236196Z",
     "iopub.status.idle": "2025-12-01T16:42:52.243541Z",
     "shell.execute_reply": "2025-12-01T16:42:52.242481Z"
    },
    "papermill": {
     "duration": 0.014827,
     "end_time": "2025-12-01T16:42:52.245197",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.230370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. RETURN -> SIGNAL MAPPING\n",
    "# ============================================================\n",
    "\n",
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray | float | list[float],\n",
    "    params: RetToSignalParameters\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert raw model predictions (expected excess returns) into a trading signal.\n",
    "\n",
    "    Mapping dasar (mode linear):\n",
    "        signal = clip( ret * signal_multiplier + 1, min_signal, max_signal )\n",
    "\n",
    "    Di mana:\n",
    "        - signal ≈ 1  : posisi netral / benchmark\n",
    "        - signal < 1  : underweight (kurang dari pasar)\n",
    "        - signal > 1  : overweight (lebih agresif dari pasar)\n",
    "\n",
    "    Args:\n",
    "        ret_arr:\n",
    "            Predicted returns (bisa scalar, list, atau numpy array).\n",
    "        params (RetToSignalParameters):\n",
    "            Parameter scaling dan clipping (min/max signal, multiplier).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            Array sinyal trading dengan shape 1D (n_samples,),\n",
    "            sudah di-clip di [min_signal, max_signal].\n",
    "            (Kalau input scalar, tetap dikembalikan array 1 elemen.)\n",
    "    \"\"\"\n",
    "    # Pastikan dalam bentuk numpy array float (1D)\n",
    "    ret_arr = np.asarray(ret_arr, dtype=float).reshape(-1)\n",
    "\n",
    "    # Sanity-check: tidak boleh ada NaN / inf\n",
    "    if not np.all(np.isfinite(ret_arr)):\n",
    "        raise ValueError(\n",
    "            \"ret_arr mengandung nilai non-finite (NaN/inf). \"\n",
    "            \"Pastikan prediksi model sudah dibersihkan dulu.\"\n",
    "        )\n",
    "\n",
    "    # Mapping linear dari return -> posisi\n",
    "    raw_signal = ret_arr * params.signal_multiplier + 1.0\n",
    "\n",
    "    # Clip supaya tidak keluar dari range yang diizinkan\n",
    "    signal = np.clip(raw_signal, params.min_signal, params.max_signal)\n",
    "\n",
    "    # (Opsional) Debug ringan saat run lokal\n",
    "    if not IS_COMP_RERUN:\n",
    "        # hanya log sangat singkat, supaya tidak flood output\n",
    "        s_min, s_max, s_mean = float(signal.min()), float(signal.max()), float(signal.mean())\n",
    "        print(f\"[DEBUG signal] min={s_min:.3f}, max={s_max:.3f}, mean={s_mean:.3f}\")\n",
    "\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b769f74",
   "metadata": {
    "papermill": {
     "duration": 0.004554,
     "end_time": "2025-12-01T16:42:52.254820",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.250266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Looking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88736768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.265920Z",
     "iopub.status.busy": "2025-12-01T16:42:52.265573Z",
     "iopub.status.idle": "2025-12-01T16:42:52.693736Z",
     "shell.execute_reply": "2025-12-01T16:42:52.692192Z"
    },
    "papermill": {
     "duration": 0.435955,
     "end_time": "2025-12-01T16:42:52.695484",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.259529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE ===\n",
      "Train shape: (9011, 98)\n",
      "Test shape : (10, 99)\n",
      "\n",
      "=== DATE RANGE ===\n",
      "Train date_id range: 0 → 9010\n",
      "Test  date_id range: 8980 → 8989\n",
      "\n",
      "=== TRAIN SAMPLE (tail 3) ===\n",
      "shape: (3, 98)\n",
      "┌─────────┬─────┬─────┬─────┬───┬───────────┬─────────────────┬────────────────┬───────────┐\n",
      "│ date_id ┆ D1  ┆ D2  ┆ D3  ┆ … ┆ V9        ┆ forward_returns ┆ risk_free_rate ┆ target    │\n",
      "│ ---     ┆ --- ┆ --- ┆ --- ┆   ┆ ---       ┆ ---             ┆ ---            ┆ ---       │\n",
      "│ i32     ┆ f64 ┆ f64 ┆ f64 ┆   ┆ f64       ┆ f64             ┆ f64            ┆ f64       │\n",
      "╞═════════╪═════╪═════╪═════╪═══╪═══════════╪═════════════════╪════════════════╪═══════════╡\n",
      "│ 9008    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.530228 ┆ -0.002897       ┆ 0.0001525      ┆ -0.003362 │\n",
      "│ 9009    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.512769 ┆ -0.027028       ┆ 0.000153       ┆ -0.027493 │\n",
      "│ 9010    ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ … ┆ -0.015503 ┆ 0.015344        ┆ 0.000153       ┆ 0.014879  │\n",
      "└─────────┴─────┴─────┴─────┴───┴───────────┴─────────────────┴────────────────┴───────────┘\n",
      "\n",
      "=== TEST SAMPLE (head 5) ===\n",
      "shape: (5, 3)\n",
      "┌─────────┬───────────┬───────────┐\n",
      "│ date_id ┆ is_scored ┆ target    │\n",
      "│ ---     ┆ ---       ┆ ---       │\n",
      "│ i32     ┆ f64       ┆ f64       │\n",
      "╞═════════╪═══════════╪═══════════╡\n",
      "│ 8980    ┆ 1.0       ┆ 0.003541  │\n",
      "│ 8981    ┆ 1.0       ┆ -0.005964 │\n",
      "│ 8982    ┆ 1.0       ┆ -0.00741  │\n",
      "│ 8983    ┆ 1.0       ┆ 0.00542   │\n",
      "│ 8984    ┆ 1.0       ┆ 0.008357  │\n",
      "└─────────┴───────────┴───────────┘\n",
      "\n",
      "=== TARGET STATS (train.target) ===\n",
      "shape: (9, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ statistic  ┆ target    │\n",
      "│ ---        ┆ ---       │\n",
      "│ str        ┆ f64       │\n",
      "╞════════════╪═══════════╡\n",
      "│ count      ┆ 9011.0    │\n",
      "│ null_count ┆ 0.0       │\n",
      "│ mean       ┆ 0.00005   │\n",
      "│ std        ┆ 0.010562  │\n",
      "│ min        ┆ -0.040582 │\n",
      "│ 25%        ┆ -0.004747 │\n",
      "│ 50%        ┆ 0.000253  │\n",
      "│ 75%        ┆ 0.005479  │\n",
      "│ max        ┆ 0.040551  │\n",
      "└────────────┴───────────┘\n",
      "\n",
      "=== TOP 10 NULL COUNTS (TRAIN) ===\n",
      "shape: (10, 2)\n",
      "┌────────┬────────┐\n",
      "│ column ┆ n_null │\n",
      "│ ---    ┆ ---    │\n",
      "│ str    ┆ u32    │\n",
      "╞════════╪════════╡\n",
      "│ E7     ┆ 6969   │\n",
      "│ V10    ┆ 6049   │\n",
      "│ S3     ┆ 5733   │\n",
      "│ M1     ┆ 5547   │\n",
      "│ M13    ┆ 5540   │\n",
      "│ M14    ┆ 5540   │\n",
      "│ M6     ┆ 5043   │\n",
      "│ V9     ┆ 4539   │\n",
      "│ S12    ┆ 3537   │\n",
      "│ M5     ┆ 3283   │\n",
      "└────────┴────────┘\n",
      "\n",
      "=== TOP 10 NULL COUNTS (TEST) ===\n",
      "shape: (10, 2)\n",
      "┌─────────┬────────┐\n",
      "│ column  ┆ n_null │\n",
      "│ ---     ┆ ---    │\n",
      "│ str     ┆ u32    │\n",
      "╞═════════╪════════╡\n",
      "│ date_id ┆ 0      │\n",
      "│ D1      ┆ 0      │\n",
      "│ D2      ┆ 0      │\n",
      "│ D3      ┆ 0      │\n",
      "│ D4      ┆ 0      │\n",
      "│ D5      ┆ 0      │\n",
      "│ D6      ┆ 0      │\n",
      "│ D7      ┆ 0      │\n",
      "│ D8      ┆ 0      │\n",
      "│ D9      ┆ 0      │\n",
      "└─────────┴────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/4241365167.py:50: DeprecationWarning: `DataFrame.melt` is deprecated. Use `unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  .melt(variable_name=\"column\", value_name=\"n_null\")\n",
      "/tmp/ipykernel_13/4241365167.py:60: DeprecationWarning: `DataFrame.melt` is deprecated. Use `unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  .melt(variable_name=\"column\", value_name=\"n_null\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. QUICK DATA CHECK: TRAIN & TEST (LOCAL ONLY)\n",
    "# ============================================================\n",
    "\n",
    "# Bagian ini hanya dijalankan saat TIDAK rerun kompetisi,\n",
    "# supaya tidak menambah waktu eksekusi saat submit.\n",
    "if not IS_COMP_RERUN:\n",
    "    # Load data mentah\n",
    "    train: pl.DataFrame = load_trainset()\n",
    "    test: pl.DataFrame  = load_testset()\n",
    "\n",
    "    print(\"=== SHAPE ===\")\n",
    "    print(\"Train shape:\", train.shape)\n",
    "    print(\"Test shape :\", test.shape)\n",
    "\n",
    "    # Range date_id untuk memastikan urut dan tidak kosong\n",
    "    print(\"\\n=== DATE RANGE ===\")\n",
    "    print(\n",
    "        \"Train date_id range:\",\n",
    "        int(train[\"date_id\"].min()),\n",
    "        \"→\",\n",
    "        int(train[\"date_id\"].max()),\n",
    "    )\n",
    "    print(\n",
    "        \"Test  date_id range:\",\n",
    "        int(test[\"date_id\"].min()),\n",
    "        \"→\",\n",
    "        int(test[\"date_id\"].max()),\n",
    "    )\n",
    "\n",
    "    # Cek beberapa baris terakhir train\n",
    "    print(\"\\n=== TRAIN SAMPLE (tail 3) ===\")\n",
    "    print(train.tail(3))\n",
    "\n",
    "    # Cek beberapa kolom penting di test (date_id, is_scored, target)\n",
    "    cols_to_show_test = [\n",
    "        c for c in [\"date_id\", \"is_scored\", \"target\"] if c in test.columns\n",
    "    ]\n",
    "    print(\"\\n=== TEST SAMPLE (head 5) ===\")\n",
    "    print(test.select(cols_to_show_test).head(5))\n",
    "\n",
    "    # Statistik dasar target di train\n",
    "    print(\"\\n=== TARGET STATS (train.target) ===\")\n",
    "    print(train.select(\"target\").describe())\n",
    "\n",
    "    # Sedikit info missing values di train\n",
    "    print(\"\\n=== TOP 10 NULL COUNTS (TRAIN) ===\")\n",
    "    null_counts_train = (\n",
    "        train.null_count()\n",
    "        .melt(variable_name=\"column\", value_name=\"n_null\")\n",
    "        .sort(\"n_null\", descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(null_counts_train)\n",
    "\n",
    "    # Dan di test\n",
    "    print(\"\\n=== TOP 10 NULL COUNTS (TEST) ===\")\n",
    "    null_counts_test = (\n",
    "        test.null_count()\n",
    "        .melt(variable_name=\"column\", value_name=\"n_null\")\n",
    "        .sort(\"n_null\", descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(null_counts_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b321e0",
   "metadata": {
    "papermill": {
     "duration": 0.004647,
     "end_time": "2025-12-01T16:42:52.705545",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.700898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating the Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90eab54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.716897Z",
     "iopub.status.busy": "2025-12-01T16:42:52.716470Z",
     "iopub.status.idle": "2025-12-01T16:42:52.881402Z",
     "shell.execute_reply": "2025-12-01T16:42:52.880647Z"
    },
    "papermill": {
     "duration": 0.172376,
     "end_time": "2025-12-01T16:42:52.882807",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.710431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FE & SPLIT SUMMARY ===\n",
      "X_train shape : (7509, 35)\n",
      "X_test  shape : (20, 35)\n",
      "y_train length: 7509\n",
      "y_test  length: 20\n",
      "Num features  : 35\n",
      "Train dates   : 1512 → 9010\n",
      "Test  dates   : 8980 → 8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1691558767.py:127: DeprecationWarning: The argument `min_periods` for `Expr.rolling_mean` is deprecated. It has been renamed to `min_samples`.\n",
      "  pl.col(col).rolling_mean(window_size=5, min_periods=1).alias(rm5_name),\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. FEATURE ENGINEERING + TRAIN/TEST SPLIT\n",
    "# ============================================================\n",
    "\n",
    "# 1) Selalu load train & test mentah di sini\n",
    "#    (jangan bergantung ke cell EDA yang hanya jalan saat not IS_COMP_RERUN)\n",
    "train_raw: pl.DataFrame = load_trainset()\n",
    "test_raw: pl.DataFrame  = load_testset()\n",
    "\n",
    "# 2) Gabungkan train & test mentah pada kolom yang sama (supaya FE konsisten)\n",
    "df_all: pl.DataFrame = join_train_test_dataframes(train_raw, test_raw)\n",
    "\n",
    "# 3) Terapkan feature engineering (U1, U2, momentum, rolling, EWM fill, drop_null)\n",
    "df_fe: pl.DataFrame = create_example_dataset(df=df_all)\n",
    "\n",
    "# 4) Kembalikan lagi ke train_fe dan test_fe berdasarkan date_id awal\n",
    "train_ids = train_raw.get_column(\"date_id\").unique()\n",
    "test_ids  = test_raw.get_column(\"date_id\").unique()\n",
    "\n",
    "train_fe: pl.DataFrame = df_fe.filter(pl.col(\"date_id\").is_in(train_ids))\n",
    "test_fe: pl.DataFrame  = df_fe.filter(pl.col(\"date_id\").is_in(test_ids))\n",
    "\n",
    "# Pastikan sudah terurut waktu\n",
    "train_fe = train_fe.sort(\"date_id\")\n",
    "test_fe  = test_fe.sort(\"date_id\")\n",
    "\n",
    "# 5) Definisikan daftar fitur (semua kolom kecuali 'date_id' dan 'target')\n",
    "FEATURES: list[str] = sorted(\n",
    "    [col for col in test_fe.columns if col not in [\"date_id\", \"target\"]]\n",
    ")\n",
    "\n",
    "# 6) Split menjadi X/y + scaling, dibungkus dalam DatasetOutput\n",
    "dataset: DatasetOutput = split_dataset(\n",
    "    train=train_fe,\n",
    "    test=test_fe,\n",
    "    features=FEATURES,\n",
    ")\n",
    "\n",
    "X_train: pl.DataFrame      = dataset.X_train\n",
    "X_test: pl.DataFrame       = dataset.X_test\n",
    "y_train: pl.Series         = dataset.y_train\n",
    "y_test: pl.Series          = dataset.y_test\n",
    "scaler: StandardScaler     = dataset.scaler\n",
    "feature_names: list[str]   = dataset.feature_names or FEATURES\n",
    "dates_train: pl.Series     = dataset.dates_train\n",
    "dates_test: pl.Series      = dataset.dates_test\n",
    "\n",
    "# (Opsional) Quick check – hanya saat lokal, supaya tidak nambah waktu di kompetisi\n",
    "if not IS_COMP_RERUN:\n",
    "    print(\"=== FE & SPLIT SUMMARY ===\")\n",
    "    print(\"X_train shape :\", X_train.shape)\n",
    "    print(\"X_test  shape :\", X_test.shape)\n",
    "    print(\"y_train length:\", y_train.len())\n",
    "    print(\"y_test  length:\", y_test.len())\n",
    "    print(\"Num features  :\", len(FEATURES))\n",
    "    print(\"Train dates   :\", int(dates_train.min()), \"→\", int(dates_train.max()))\n",
    "    print(\"Test  dates   :\", int(dates_test.min()),  \"→\", int(dates_test.max()))\n",
    "\n",
    "    assert X_train.height == y_train.len(), \"X_train dan y_train tidak sebaris!\"\n",
    "    assert X_test.height == y_test.len(), \"X_test dan y_test tidak sebaris!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8aedcd",
   "metadata": {
    "papermill": {
     "duration": 0.00543,
     "end_time": "2025-12-01T16:42:52.893512",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.888082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fitting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b92d4ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:52.905528Z",
     "iopub.status.busy": "2025-12-01T16:42:52.905230Z",
     "iopub.status.idle": "2025-12-01T16:42:53.239013Z",
     "shell.execute_reply": "2025-12-01T16:42:53.238046Z"
    },
    "papermill": {
     "duration": 0.341678,
     "end_time": "2025-12-01T16:42:53.240421",
     "exception": false,
     "start_time": "2025-12-01T16:42:52.898743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ElasticNet Baseline Fitted ===\n",
      "Best alpha (CV) : 0.000305\n",
      "L1 ratio        : 0.5\n",
      "CV folds        : 10\n",
      "Train R²        : 0.014019\n",
      "Train MSE       : 1.213405e-04\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. FIT ELASTICNET BASELINE (WITH CV)\n",
    "# ============================================================\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Sklearn lebih aman kalau dikasih numpy array\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "\n",
    "# Siapkan argumen untuk ElasticNetCV dari enet_params + tambahan\n",
    "enet_cv_kwargs = asdict(enet_params).copy()\n",
    "enet_cv_kwargs.update(\n",
    "    {\n",
    "        \"fit_intercept\": True,\n",
    "        \"random_state\": SEED,\n",
    "        \"n_jobs\": -1,   # pakai semua core yang tersedia\n",
    "    }\n",
    ")\n",
    "\n",
    "# 1) Cross-validated ElasticNet untuk cari alpha terbaik\n",
    "model_cv: ElasticNetCV = ElasticNetCV(**enet_cv_kwargs)\n",
    "model_cv.fit(X_train_np, y_train_np)\n",
    "\n",
    "best_alpha: float = float(model_cv.alpha_)\n",
    "\n",
    "# 2) Fit final ElasticNet dengan alpha terbaik\n",
    "model: ElasticNet = ElasticNet(\n",
    "    alpha=best_alpha,\n",
    "    l1_ratio=enet_params.l1_ratio,\n",
    "    max_iter=enet_params.max_iter,\n",
    "    fit_intercept=True,\n",
    "    random_state=SEED,\n",
    ")\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Quick sanity check: performa di training\n",
    "# ------------------------------------------------------------\n",
    "y_pred_train = model.predict(X_train_np)\n",
    "r2_train = r2_score(y_train_np, y_pred_train)\n",
    "mse_train = mean_squared_error(y_train_np, y_pred_train)\n",
    "\n",
    "print(\"=== ElasticNet Baseline Fitted ===\")\n",
    "print(f\"Best alpha (CV) : {best_alpha:.6f}\")\n",
    "print(f\"L1 ratio        : {enet_params.l1_ratio}\")\n",
    "print(f\"CV folds        : {enet_params.cv}\")\n",
    "print(f\"Train R²        : {r2_train:.6f}\")\n",
    "print(f\"Train MSE       : {mse_train:.6e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4266c68",
   "metadata": {
    "papermill": {
     "duration": 0.004884,
     "end_time": "2025-12-01T16:42:53.250685",
     "exception": false,
     "start_time": "2025-12-01T16:42:53.245801",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function via Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28bb7ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:53.262443Z",
     "iopub.status.busy": "2025-12-01T16:42:53.261852Z",
     "iopub.status.idle": "2025-12-01T16:42:53.269442Z",
     "shell.execute_reply": "2025-12-01T16:42:53.268625Z"
    },
    "papermill": {
     "duration": 0.015117,
     "end_time": "2025-12-01T16:42:53.270928",
     "exception": false,
     "start_time": "2025-12-01T16:42:53.255811",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. PREDICTION FUNCTION FOR KAGGLE EVALUATION API\n",
    "# ============================================================\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Fungsi prediksi yang dipanggil oleh Kaggle Evaluation API.\n",
    "\n",
    "    Alur:\n",
    "    1) Rename kolom target (lagged_forward_returns -> target) jika perlu.\n",
    "    2) Terapkan feature engineering (create_example_dataset).\n",
    "    3) Pilih fitur sesuai FEATURES dan lakukan scaling dengan scaler yang sudah di-fit.\n",
    "    4) Prediksi expected excess return pakai model.\n",
    "    5) Konversi return -> sinyal trading via convert_ret_to_signal.\n",
    "    6) Return satu nilai float (sinyal) untuk batch pertama.\n",
    "\n",
    "    Catatan:\n",
    "    - DefaultInferenceServer biasanya memanggil fungsi ini dengan\n",
    "      1 baris per call, tapi kita tetap handle jika >1 baris.\n",
    "    \"\"\"\n",
    "    # 1. Pastikan kolom target ada\n",
    "    if \"target\" not in test.columns:\n",
    "        if \"lagged_forward_returns\" in test.columns:\n",
    "            test = test.rename({\"lagged_forward_returns\": \"target\"})\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Test dataframe harus memiliki kolom 'target' atau 'lagged_forward_returns'.\"\n",
    "            )\n",
    "\n",
    "    # 2. Pastikan 'date_id' ada\n",
    "    if \"date_id\" not in test.columns:\n",
    "        raise KeyError(\"Kolom 'date_id' wajib ada di dataframe test.\")\n",
    "\n",
    "    # 3. Feature engineering (U1, U2, imputasi EWM, subset fitur, drop_null)\n",
    "    df = create_example_dataset(test)\n",
    "\n",
    "    # Jika setelah FE tidak ada baris (misal semua null dan ter-drop),\n",
    "    # kembalikan sinyal netral 1.0 agar tidak crash saat evaluasi.\n",
    "    if df.height == 0:\n",
    "        return float(1.0)\n",
    "\n",
    "    # 4. Ambil hanya fitur yang sudah didefinisikan di training\n",
    "    X_test = df.select(FEATURES)\n",
    "\n",
    "    # Sanity-check: pastikan semua fitur ada\n",
    "    missing_feats = [f for f in FEATURES if f not in X_test.columns]\n",
    "    if missing_feats:\n",
    "        raise KeyError(\n",
    "            f\"Fitur berikut hilang di data FE test: {missing_feats}\"\n",
    "        )\n",
    "\n",
    "    # 5. Sklearn pakai numpy array\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    X_test_scaled_np = scaler.transform(X_test_np)\n",
    "\n",
    "    # 6. Prediksi expected excess return\n",
    "    raw_pred = model.predict(X_test_scaled_np)  # array shape (n_samples,)\n",
    "\n",
    "    # 7. Konversi ke sinyal trading\n",
    "    signal_arr = convert_ret_to_signal(raw_pred, ret_signal_params)\n",
    "\n",
    "    # 8. Ambil satu nilai (biasanya satu baris per call)\n",
    "    return float(signal_arr[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20821e3e",
   "metadata": {
    "papermill": {
     "duration": 0.004804,
     "end_time": "2025-12-01T16:42:53.281037",
     "exception": false,
     "start_time": "2025-12-01T16:42:53.276233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Launch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a58964e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-01T16:42:53.292614Z",
     "iopub.status.busy": "2025-12-01T16:42:53.292271Z",
     "iopub.status.idle": "2025-12-01T16:42:53.592948Z",
     "shell.execute_reply": "2025-12-01T16:42:53.592059Z"
    },
    "papermill": {
     "duration": 0.30877,
     "end_time": "2025-12-01T16:42:53.594786",
     "exception": false,
     "start_time": "2025-12-01T16:42:53.286016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in LOCAL GATEWAY mode for debugging...\n",
      "Using input dir: /kaggle/input/hull-tactical-market-prediction\n",
      "\n",
      "submission.parquet generated at: /kaggle/working/submission.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/1691558767.py:127: DeprecationWarning: The argument `min_periods` for `Expr.rolling_mean` is deprecated. It has been renamed to `min_samples`.\n",
      "  pl.col(col).rolling_mean(window_size=5, min_periods=1).alias(rm5_name),\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. START KAGGLE EVALUATION SERVER\n",
    "# ============================================================\n",
    "\n",
    "inference_server = kei.DefaultInferenceServer(predict)\n",
    "\n",
    "if IS_COMP_RERUN:\n",
    "    # Mode ini dipakai saat SUBMIT notebook ke kompetisi\n",
    "    print(\"Detected competition rerun environment. Starting inference server...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Mode lokal / saat run manual di Notebook\n",
    "    print(\"Running in LOCAL GATEWAY mode for debugging...\")\n",
    "    print(f\"Using input dir: {INPUT_DIR}\")\n",
    "    inference_server.run_local_gateway((str(INPUT_DIR),))\n",
    "\n",
    "    # Setelah lokal gateway selesai, cek apakah submission.parquet sudah dibuat\n",
    "    sub_path = Path(\"submission.parquet\")\n",
    "    if sub_path.exists():\n",
    "        print(\"\\nsubmission.parquet generated at:\", sub_path.resolve())\n",
    "    else:\n",
    "        print(\"\\nWARNING: submission.parquet not found in working directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 19.242556,
   "end_time": "2025-12-01T16:42:54.622214",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-01T16:42:35.379658",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
