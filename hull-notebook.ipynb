{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836e4f35",
   "metadata": {
    "papermill": {
     "duration": 0.00671,
     "end_time": "2025-12-04T14:17:49.246915",
     "exception": false,
     "start_time": "2025-12-04T14:17:49.240205",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a73307",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:17:49.259346Z",
     "iopub.status.busy": "2025-12-04T14:17:49.259014Z",
     "iopub.status.idle": "2025-12-04T14:17:54.398093Z",
     "shell.execute_reply": "2025-12-04T14:17:54.396922Z"
    },
    "papermill": {
     "duration": 5.147553,
     "end_time": "2025-12-04T14:17:54.400112",
     "exception": false,
     "start_time": "2025-12-04T14:17:49.252559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. Imports & Global Setup\n",
    "# ============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "# ---------- Standard Library ----------\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "# ---------- Third-party ----------\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, ElasticNetCV, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tqdm.auto import tqdm  # progress bar di notebook\n",
    "\n",
    "import kaggle_evaluation.default_inference_server as kei\n",
    "# nanti dipakai sebagai: kei.DefaultInferenceServer(predict)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Reproducibility\n",
    "# ============================================================\n",
    "SEED: int = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "# kalau nanti pakai model lain (mis. RandomizedSearch, dsb.), pakai SEED ini juga\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Polars / Display Config (QoL)\n",
    "# ============================================================\n",
    "# Supaya print DataFrame tidak terlalu panjang di output\n",
    "pl.Config.set_tbl_cols(20)\n",
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Logging & Warning Setup\n",
    "# ============================================================\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s][%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logger.info(\"Environment initialised | SEED=%d\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa927bd",
   "metadata": {
    "papermill": {
     "duration": 0.005875,
     "end_time": "2025-12-04T14:17:54.412140",
     "exception": false,
     "start_time": "2025-12-04T14:17:54.406265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Project Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f3076b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-04T14:17:54.425013Z",
     "iopub.status.busy": "2025-12-04T14:17:54.424477Z",
     "iopub.status.idle": "2025-12-04T14:17:54.434887Z",
     "shell.execute_reply": "2025-12-04T14:17:54.433745Z"
    },
    "papermill": {
     "duration": 0.018581,
     "end_time": "2025-12-04T14:17:54.436471",
     "exception": false,
     "start_time": "2025-12-04T14:17:54.417890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/hull-tactical-market-prediction/train.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/test.csv\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_inference_server.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/default_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/hull-tactical-market-prediction/kaggle_evaluation/core/generated/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d905ac02",
   "metadata": {
    "papermill": {
     "duration": 0.005728,
     "end_time": "2025-12-04T14:17:54.448003",
     "exception": false,
     "start_time": "2025-12-04T14:17:54.442275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f47bd8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:17:54.461550Z",
     "iopub.status.busy": "2025-12-04T14:17:54.461214Z",
     "iopub.status.idle": "2025-12-04T14:18:00.751696Z",
     "shell.execute_reply": "2025-12-04T14:18:00.750778Z"
    },
    "papermill": {
     "duration": 6.299406,
     "end_time": "2025-12-04T14:18:00.753404",
     "exception": false,
     "start_time": "2025-12-04T14:17:54.453998",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 1. IMPORTS & GLOBAL SETUP\n",
    "# ============================================================\n",
    "from __future__ import annotations\n",
    "\n",
    "# ---- Standard library ----\n",
    "import os\n",
    "import datetime\n",
    "import random\n",
    "import warnings\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "\n",
    "# ---- Third-party ----\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.linear_model import (\n",
    "    ElasticNet,\n",
    "    ElasticNetCV,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Optional: tree-based model (kalau nanti kamu mau coba ensemble non-linear)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except ImportError:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "import kaggle_evaluation.default_inference_server as kei\n",
    "\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "SEED: int = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "# ---- Polars & Logging QoL ----\n",
    "pl.Config.set_tbl_cols(20)\n",
    "pl.Config.set_tbl_rows(20)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s][%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "logger.info(\"Environment initialised | SEED=%d | HAS_LGBM=%s\", SEED, HAS_LGBM)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. PROJECT DIRECTORY STRUCTURE & CONFIG\n",
    "# ============================================================\n",
    "\n",
    "# Nama kompetisi (dipakai untuk bikin folder kerja terpisah)\n",
    "COMP_NAME: str = \"hull-tactical-market-prediction\"\n",
    "\n",
    "# Nama eksperimen (bisa kamu ganti saat coba konfigurasi lain)\n",
    "EXPERIMENT_NAME: str = f\"enet_v1_{datetime.datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "\n",
    "# ---- Input & Working Dirs ----\n",
    "INPUT_DIR: Path = Path(\"/kaggle/input\") / COMP_NAME\n",
    "WORK_DIR: Path  = Path(\"/kaggle/working\") / COMP_NAME\n",
    "\n",
    "WORK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File data utama\n",
    "TRAIN_PATH: Path = INPUT_DIR / \"train.csv\"\n",
    "TEST_PATH: Path  = INPUT_DIR / \"test.csv\"\n",
    "\n",
    "# Folder resmi dari Kaggle Evaluation API (source & copy ke working jika perlu)\n",
    "KAGGLE_EVAL_SRC: Path  = INPUT_DIR / \"kaggle_evaluation\"\n",
    "KAGGLE_EVAL_WORK: Path = WORK_DIR / \"kaggle_evaluation\"\n",
    "KAGGLE_EVAL_WORK.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Output structure (untuk hasil eksperimen) ----\n",
    "OUT_DIR: Path        = WORK_DIR / \"outputs\"\n",
    "MODEL_DIR: Path      = OUT_DIR / \"models\"       # simpan model, scaler, dsb.\n",
    "FEATURE_DIR: Path    = OUT_DIR / \"features\"     # simpan dataset hasil FE (opsional)\n",
    "LOG_DIR: Path        = OUT_DIR / \"logs\"         # catatan eksperimen, metrik\n",
    "SUBMISSION_DIR: Path = OUT_DIR / \"submissions\"  # submission lokal untuk dicek\n",
    "\n",
    "for p in [OUT_DIR, MODEL_DIR, FEATURE_DIR, LOG_DIR, SUBMISSION_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Flag environment: apakah ini run dalam mode kompetisi (rerun) atau lokal\n",
    "IS_COMP_RERUN: bool = os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\") is not None\n",
    "logger.info(\"IS_COMP_RERUN = %s\", IS_COMP_RERUN)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. RETURNS -> SIGNAL CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SignalConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    min_signal / max_signal : batas posisi (0 = cash, 2 = 2x leverage)\n",
    "    multiplier              : skala sensitivitas sinyal terhadap prediksi return\n",
    "    \"\"\"\n",
    "    min_signal: float = 0.0\n",
    "    max_signal: float = 2.0\n",
    "    multiplier: float = 400.0\n",
    "\n",
    "\n",
    "# Beberapa preset sinyal yang nanti bisa kamu pilih via validasi Sharpe\n",
    "SIGNAL_PRESETS: Dict[str, SignalConfig] = {\n",
    "    \"baseline\":     SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=400.0),\n",
    "    \"conservative\": SignalConfig(min_signal=0.5, max_signal=1.5, multiplier=200.0),\n",
    "    \"aggressive\":   SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=600.0),\n",
    "}\n",
    "\n",
    "ACTIVE_SIGNAL_KEY: str = \"baseline\"  # bisa kamu ubah setelah ada hasil CV\n",
    "SIGNAL_CFG: SignalConfig = SIGNAL_PRESETS[ACTIVE_SIGNAL_KEY]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. MODEL & EVALUATION CONFIGS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi utama untuk model ElasticNet baseline.\n",
    "    \"\"\"\n",
    "    cv_folds: int = 10\n",
    "    l1_ratio: float = 0.5\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: np.logspace(-4, 2, 100)\n",
    "    )\n",
    "    max_iter: int = 1_000_000\n",
    "    random_state: int = SEED\n",
    "\n",
    "ENET_CFG = ModelConfig()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi evaluasi internal (Sharpe, validasi time-series).\n",
    "    \"\"\"\n",
    "    n_folds: int = 5                 # jumlah fold time-series CV\n",
    "    val_fraction: float = 0.2        # porsi akhir data untuk pure hold-out (opsional)\n",
    "    sharpe_annualization_factor: float = float(np.sqrt(252.0))  # 252 hari bursa\n",
    "\n",
    "EVAL_CFG = EvaluationConfig()\n",
    "logger.info(\n",
    "    \"Configs ready | ENET l1_ratio=%.3f | n_folds=%d | signal_preset=%s\",\n",
    "    ENET_CFG.l1_ratio,\n",
    "    EVAL_CFG.n_folds,\n",
    "    ACTIVE_SIGNAL_KEY,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa5bf313",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:00.767414Z",
     "iopub.status.busy": "2025-12-04T14:18:00.765850Z",
     "iopub.status.idle": "2025-12-04T14:18:00.778112Z",
     "shell.execute_reply": "2025-12-04T14:18:00.776966Z"
    },
    "papermill": {
     "duration": 0.020895,
     "end_time": "2025-12-04T14:18:00.780153",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.759258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORK_DIR : /kaggle/working/hull-tactical-market-prediction\n",
      "TRAIN_PATH exists: True\n",
      "TEST_PATH exists : True\n",
      "Signal config    : SignalConfig(min_signal=0.0, max_signal=2.0, multiplier=400.0)\n",
      "Model config     : ModelConfig(cv_folds=10, l1_ratio=0.5, alphas=array([1.00000000e-04, 1.14975700e-04, 1.32194115e-04, 1.51991108e-04,\n",
      "       1.74752840e-04, 2.00923300e-04, 2.31012970e-04, 2.65608778e-04,\n",
      "       3.05385551e-04, 3.51119173e-04, 4.03701726e-04, 4.64158883e-04,\n",
      "       5.33669923e-04, 6.13590727e-04, 7.05480231e-04, 8.11130831e-04,\n",
      "       9.32603347e-04, 1.07226722e-03, 1.23284674e-03, 1.41747416e-03,\n",
      "       1.62975083e-03, 1.87381742e-03, 2.15443469e-03, 2.47707636e-03,\n",
      "       2.84803587e-03, 3.27454916e-03, 3.76493581e-03, 4.32876128e-03,\n",
      "       4.97702356e-03, 5.72236766e-03, 6.57933225e-03, 7.56463328e-03,\n",
      "       8.69749003e-03, 1.00000000e-02, 1.14975700e-02, 1.32194115e-02,\n",
      "       1.51991108e-02, 1.74752840e-02, 2.00923300e-02, 2.31012970e-02,\n",
      "       2.65608778e-02, 3.05385551e-02, 3.51119173e-02, 4.03701726e-02,\n",
      "       4.64158883e-02, 5.33669923e-02, 6.13590727e-02, 7.05480231e-02,\n",
      "       8.11130831e-02, 9.32603347e-02, 1.07226722e-01, 1.23284674e-01,\n",
      "       1.41747416e-01, 1.62975083e-01, 1.87381742e-01, 2.15443469e-01,\n",
      "       2.47707636e-01, 2.84803587e-01, 3.27454916e-01, 3.76493581e-01,\n",
      "       4.32876128e-01, 4.97702356e-01, 5.72236766e-01, 6.57933225e-01,\n",
      "       7.56463328e-01, 8.69749003e-01, 1.00000000e+00, 1.14975700e+00,\n",
      "       1.32194115e+00, 1.51991108e+00, 1.74752840e+00, 2.00923300e+00,\n",
      "       2.31012970e+00, 2.65608778e+00, 3.05385551e+00, 3.51119173e+00,\n",
      "       4.03701726e+00, 4.64158883e+00, 5.33669923e+00, 6.13590727e+00,\n",
      "       7.05480231e+00, 8.11130831e+00, 9.32603347e+00, 1.07226722e+01,\n",
      "       1.23284674e+01, 1.41747416e+01, 1.62975083e+01, 1.87381742e+01,\n",
      "       2.15443469e+01, 2.47707636e+01, 2.84803587e+01, 3.27454916e+01,\n",
      "       3.76493581e+01, 4.32876128e+01, 4.97702356e+01, 5.72236766e+01,\n",
      "       6.57933225e+01, 7.56463328e+01, 8.69749003e+01, 1.00000000e+02]), max_iter=1000000, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "print(\"WORK_DIR :\", WORK_DIR)\n",
    "print(\"TRAIN_PATH exists:\", TRAIN_PATH.exists())\n",
    "print(\"TEST_PATH exists :\", TEST_PATH.exists())\n",
    "print(\"Signal config    :\", SIGNAL_CFG)\n",
    "print(\"Model config     :\", ENET_CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd2f0c7",
   "metadata": {
    "papermill": {
     "duration": 0.005383,
     "end_time": "2025-12-04T14:18:00.791420",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.786037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataclasses Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87aabd17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:00.805294Z",
     "iopub.status.busy": "2025-12-04T14:18:00.804941Z",
     "iopub.status.idle": "2025-12-04T14:18:00.822978Z",
     "shell.execute_reply": "2025-12-04T14:18:00.821927Z"
    },
    "papermill": {
     "duration": 0.026617,
     "end_time": "2025-12-04T14:18:00.824600",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.797983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5. DATACLASSES HELPERS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    \"\"\"\n",
    "    Paket hasil preprocessing dataset.\n",
    "\n",
    "    - X_train, y_train : data untuk melatih model\n",
    "    - X_test,  y_test  : data untuk evaluasi (mock test / hold-out)\n",
    "    - scaler           : StandardScaler yang sudah di-fit pada X_train\n",
    "    - feature_names    : daftar nama fitur (opsional, berguna untuk debugging/analisis)\n",
    "    \"\"\"\n",
    "    X_train: pl.DataFrame\n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "    feature_names: list[str] | None = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ElasticNetParameters:\n",
    "    \"\"\"\n",
    "    Parameter yang dipakai ketika membangun ElasticNetCV / ElasticNet.\n",
    "\n",
    "    Default-nya diambil dari ENET_CFG (ModelConfig) supaya konsisten\n",
    "    dengan konfigurasi global, tapi tetap bisa dioverride kalau perlu.\n",
    "    \"\"\"\n",
    "    l1_ratio: float = ENET_CFG.l1_ratio\n",
    "    cv: int = ENET_CFG.cv_folds\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: ENET_CFG.alphas.copy()\n",
    "    )\n",
    "    max_iter: int = ENET_CFG.max_iter\n",
    "    random_state: int = ENET_CFG.random_state\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not (0.0 <= self.l1_ratio <= 1.0):\n",
    "            raise ValueError(\n",
    "                \"ElasticNet l1_ratio harus berada di dalam interval [0, 1].\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    Default diambil dari SIGNAL_CFG supaya selaras dengan konfigurasi global.\n",
    "    \"\"\"\n",
    "    signal_multiplier: float = SIGNAL_CFG.multiplier\n",
    "    min_signal: float = SIGNAL_CFG.min_signal\n",
    "    max_signal: float = SIGNAL_CFG.max_signal\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.min_signal >= self.max_signal:\n",
    "            raise ValueError(\n",
    "                \"min_signal harus lebih kecil daripada max_signal.\"\n",
    "            )\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5b. EXPERIMENT & CV RESULT STRUCTS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ExperimentConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi satu eksperimen penuh:\n",
    "    - feature_set_name  : nama subset fitur / skenario FE yang dipakai\n",
    "    - enet_params       : parameter model ElasticNet\n",
    "    - signal_params     : parameter konversi return -> signal\n",
    "    - use_gbm           : placeholder kalau nanti mau ensemble dengan tree model\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    feature_set_name: str\n",
    "    enet_params: ElasticNetParameters\n",
    "    signal_params: RetToSignalParameters\n",
    "    use_gbm: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    \"\"\"\n",
    "    Hasil evaluasi satu fold time-series.\n",
    "    \"\"\"\n",
    "    fold_idx: int\n",
    "    train_start_date: int\n",
    "    train_end_date: int\n",
    "    val_start_date: int\n",
    "    val_end_date: int\n",
    "    sharpe: float\n",
    "    mean_return: float\n",
    "    vol_return: float\n",
    "    n_train: int\n",
    "    n_val: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CVSummary:\n",
    "    \"\"\"\n",
    "    Ringkasan hasil cross-validation untuk satu ExperimentConfig.\n",
    "\n",
    "    - fold_results : daftar hasil per fold\n",
    "    - sharpe_mean  : rata-rata Sharpe antar fold\n",
    "    - sharpe_std   : standar deviasi Sharpe antar fold (stabilitas)\n",
    "    \"\"\"\n",
    "    experiment_name: str\n",
    "    fold_results: list[FoldResult]\n",
    "\n",
    "    @property\n",
    "    def sharpe_mean(self) -> float:\n",
    "        if not self.fold_results:\n",
    "            return float(\"nan\")\n",
    "        return float(np.mean([fr.sharpe for fr in self.fold_results]))\n",
    "\n",
    "    @property\n",
    "    def sharpe_std(self) -> float:\n",
    "        if not self.fold_results:\n",
    "            return float(\"nan\")\n",
    "        return float(np.std([fr.sharpe for fr in self.fold_results], ddof=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a1959",
   "metadata": {
    "papermill": {
     "duration": 0.005498,
     "end_time": "2025-12-04T14:18:00.837120",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.831622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0e47814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:00.850867Z",
     "iopub.status.busy": "2025-12-04T14:18:00.850288Z",
     "iopub.status.idle": "2025-12-04T14:18:00.875766Z",
     "shell.execute_reply": "2025-12-04T14:18:00.874762Z"
    },
    "papermill": {
     "duration": 0.034755,
     "end_time": "2025-12-04T14:18:00.877560",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.842805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet params: ElasticNetParameters(l1_ratio=0.5, cv=10, alphas=array([1.00000000e-04, 1.14975700e-04, 1.32194115e-04, 1.51991108e-04,\n",
      "       1.74752840e-04, 2.00923300e-04, 2.31012970e-04, 2.65608778e-04,\n",
      "       3.05385551e-04, 3.51119173e-04, 4.03701726e-04, 4.64158883e-04,\n",
      "       5.33669923e-04, 6.13590727e-04, 7.05480231e-04, 8.11130831e-04,\n",
      "       9.32603347e-04, 1.07226722e-03, 1.23284674e-03, 1.41747416e-03,\n",
      "       1.62975083e-03, 1.87381742e-03, 2.15443469e-03, 2.47707636e-03,\n",
      "       2.84803587e-03, 3.27454916e-03, 3.76493581e-03, 4.32876128e-03,\n",
      "       4.97702356e-03, 5.72236766e-03, 6.57933225e-03, 7.56463328e-03,\n",
      "       8.69749003e-03, 1.00000000e-02, 1.14975700e-02, 1.32194115e-02,\n",
      "       1.51991108e-02, 1.74752840e-02, 2.00923300e-02, 2.31012970e-02,\n",
      "       2.65608778e-02, 3.05385551e-02, 3.51119173e-02, 4.03701726e-02,\n",
      "       4.64158883e-02, 5.33669923e-02, 6.13590727e-02, 7.05480231e-02,\n",
      "       8.11130831e-02, 9.32603347e-02, 1.07226722e-01, 1.23284674e-01,\n",
      "       1.41747416e-01, 1.62975083e-01, 1.87381742e-01, 2.15443469e-01,\n",
      "       2.47707636e-01, 2.84803587e-01, 3.27454916e-01, 3.76493581e-01,\n",
      "       4.32876128e-01, 4.97702356e-01, 5.72236766e-01, 6.57933225e-01,\n",
      "       7.56463328e-01, 8.69749003e-01, 1.00000000e+00, 1.14975700e+00,\n",
      "       1.32194115e+00, 1.51991108e+00, 1.74752840e+00, 2.00923300e+00,\n",
      "       2.31012970e+00, 2.65608778e+00, 3.05385551e+00, 3.51119173e+00,\n",
      "       4.03701726e+00, 4.64158883e+00, 5.33669923e+00, 6.13590727e+00,\n",
      "       7.05480231e+00, 8.11130831e+00, 9.32603347e+00, 1.07226722e+01,\n",
      "       1.23284674e+01, 1.41747416e+01, 1.62975083e+01, 1.87381742e+01,\n",
      "       2.15443469e+01, 2.47707636e+01, 2.84803587e+01, 3.27454916e+01,\n",
      "       3.76493581e+01, 4.32876128e+01, 4.97702356e+01, 5.72236766e+01,\n",
      "       6.57933225e+01, 7.56463328e+01, 8.69749003e+01, 1.00000000e+02]), max_iter=1000000, random_state=42, n_jobs=-1, fit_intercept=True)\n",
      "Signal params    : RetToSignalParameters(signal_multiplier=400.0, min_signal=0.0, max_signal=2.0)\n",
      "GBM params       : GBMParameters(num_leaves=31, max_depth=-1, learning_rate=0.03, n_estimators=500, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.0, reg_lambda=1.0, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. DATACLASSES HELPERS + PARAMETER OBJECTS\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class DatasetOutput:\n",
    "    \"\"\"\n",
    "    Paket hasil preprocessing dataset.\n",
    "\n",
    "    - X_train, y_train : data untuk melatih model\n",
    "    - X_test,  y_test  : data untuk evaluasi (mock test / hold-out)\n",
    "    - scaler           : StandardScaler yang sudah di-fit pada X_train\n",
    "    - feature_names    : daftar nama fitur (opsional, untuk debugging/analisis)\n",
    "    - dates_train      : date_id untuk baris X_train (opsional, untuk TS-CV)\n",
    "    - dates_test       : date_id untuk baris X_test  (opsional, untuk TS-CV)\n",
    "    \"\"\"\n",
    "    X_train: pl.DataFrame\n",
    "    X_test: pl.DataFrame\n",
    "    y_train: pl.Series\n",
    "    y_test: pl.Series\n",
    "    scaler: StandardScaler\n",
    "    feature_names: list[str] | None = None\n",
    "    dates_train: pl.Series | None = None\n",
    "    dates_test: pl.Series | None = None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ElasticNetParameters:\n",
    "    \"\"\"\n",
    "    Parameter yang dipakai ketika membangun ElasticNetCV / ElasticNet.\n",
    "\n",
    "    Default diambil dari ENET_CFG (ModelConfig) supaya konsisten dengan\n",
    "    konfigurasi global, tapi bisa dioverride kalau perlu.\n",
    "    \"\"\"\n",
    "    l1_ratio: float = ENET_CFG.l1_ratio\n",
    "    cv: int = ENET_CFG.cv_folds\n",
    "    alphas: np.ndarray = field(\n",
    "        default_factory=lambda: ENET_CFG.alphas.copy()\n",
    "    )\n",
    "    max_iter: int = ENET_CFG.max_iter\n",
    "    random_state: int = ENET_CFG.random_state\n",
    "    n_jobs: int = -1\n",
    "    fit_intercept: bool = True\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if not (0.0 <= self.l1_ratio <= 1.0):\n",
    "            raise ValueError(\n",
    "                \"ElasticNet l1_ratio harus berada di dalam interval [0, 1].\"\n",
    "            )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    \"\"\"\n",
    "    Parameter untuk mengubah prediksi return -> sinyal trading harian.\n",
    "\n",
    "    Default diambil dari SIGNAL_CFG supaya selaras dengan konfigurasi global.\n",
    "    \"\"\"\n",
    "    signal_multiplier: float = SIGNAL_CFG.multiplier\n",
    "    min_signal: float = SIGNAL_CFG.min_signal\n",
    "    max_signal: float = SIGNAL_CFG.max_signal\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        if self.min_signal >= self.max_signal:\n",
    "            raise ValueError(\n",
    "                \"min_signal harus lebih kecil daripada max_signal.\"\n",
    "            )\n",
    "        if self.signal_multiplier <= 0:\n",
    "            raise ValueError(\"signal_multiplier harus > 0.\")\n",
    "\n",
    "\n",
    "# ---- GBMParameters: hanya aktif kalau LightGBM tersedia ----\n",
    "if HAS_LGBM:\n",
    "    @dataclass(frozen=True)\n",
    "    class GBMParameters:\n",
    "        \"\"\"\n",
    "        Parameter untuk model tree-based (LightGBM) yang akan kita gunakan\n",
    "        sebagai pelengkap model linear (ElasticNet/Ridge) kalau HAS_LGBM=True.\n",
    "        Default-nya dibuat eksplisit di sini supaya tidak bergantung pada GBM_CFG.\n",
    "        \"\"\"\n",
    "        num_leaves: int = 31\n",
    "        max_depth: int = -1          # -1 = unlimited depth\n",
    "        learning_rate: float = 0.03\n",
    "        n_estimators: int = 500\n",
    "        subsample: float = 0.8       # row subsampling\n",
    "        colsample_bytree: float = 0.8\n",
    "        reg_alpha: float = 0.0\n",
    "        reg_lambda: float = 1.0\n",
    "        random_state: int = SEED\n",
    "else:\n",
    "    GBMParameters = None  # type: ignore\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5b. EXPERIMENT & CV RESULT STRUCTS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ExperimentConfig:\n",
    "    \"\"\"\n",
    "    Konfigurasi satu eksperimen penuh:\n",
    "    - name             : nama unik eksperimen\n",
    "    - feature_set_name : nama subset fitur / skenario FE yang dipakai\n",
    "    - enet_params      : parameter model ElasticNet\n",
    "    - signal_params    : parameter konversi return -> signal\n",
    "    - use_gbm          : apakah akan menambahkan model GBM untuk ensemble\n",
    "    \"\"\"\n",
    "    name: str\n",
    "    feature_set_name: str\n",
    "    enet_params: ElasticNetParameters\n",
    "    signal_params: RetToSignalParameters\n",
    "    use_gbm: bool = False\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FoldResult:\n",
    "    \"\"\"\n",
    "    Hasil evaluasi satu fold time-series.\n",
    "    \"\"\"\n",
    "    fold_idx: int\n",
    "    train_start_date: int\n",
    "    train_end_date: int\n",
    "    val_start_date: int\n",
    "    val_end_date: int\n",
    "    sharpe: float\n",
    "    mean_return: float\n",
    "    vol_return: float\n",
    "    n_train: int\n",
    "    n_val: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CVSummary:\n",
    "    \"\"\"\n",
    "    Ringkasan hasil cross-validation untuk satu ExperimentConfig.\n",
    "    \"\"\"\n",
    "    experiment_name: str\n",
    "    fold_results: list[FoldResult]\n",
    "\n",
    "    @property\n",
    "    def sharpe_mean(self) -> float:\n",
    "        return float(np.mean([fr.sharpe for fr in self.fold_results])) if self.fold_results else float(\"nan\")\n",
    "\n",
    "    @property\n",
    "    def sharpe_std(self) -> float:\n",
    "        return float(np.std([fr.sharpe for fr in self.fold_results], ddof=1)) if self.fold_results else float(\"nan\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Instansiasi objek parameter utama yang akan dipakai di pipeline\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "ret_signal_params = RetToSignalParameters()\n",
    "enet_params       = ElasticNetParameters()\n",
    "gbm_params: Optional[\"GBMParameters\"] = GBMParameters() if HAS_LGBM and GBMParameters is not None else None\n",
    "\n",
    "print(\"ElasticNet params:\", enet_params)\n",
    "print(\"Signal params    :\", ret_signal_params)\n",
    "print(\"GBM params       :\", gbm_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e61c28",
   "metadata": {
    "papermill": {
     "duration": 0.005694,
     "end_time": "2025-12-04T14:18:00.889233",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.883539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset Loading/Creating Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727cb9ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:00.902419Z",
     "iopub.status.busy": "2025-12-04T14:18:00.902056Z",
     "iopub.status.idle": "2025-12-04T14:18:00.925033Z",
     "shell.execute_reply": "2025-12-04T14:18:00.924029Z"
    },
    "papermill": {
     "duration": 0.031695,
     "end_time": "2025-12-04T14:18:00.926644",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.894949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. DATA LOADING & PREPROCESSING HELPERS\n",
    "# ============================================================\n",
    "\n",
    "def load_trainset(path: Path = TRAIN_PATH, drop_last_n: int = 10) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dan praproses training dataset\n",
    "\n",
    "    - Mengganti nama kolom target menjadi 'target'\n",
    "      (market_forward_excess_returns -> target).\n",
    "    - Meng-cast semua kolom selain 'date_id' ke Float64.\n",
    "    - Sorting berdasarkan date_id.\n",
    "    - Opsional: membuang N baris terakhir (drop_last_n) untuk\n",
    "      menghindari kebocoran saat mock test.\n",
    "\n",
    "    Args:\n",
    "        path (Path): lokasi file train.csv.\n",
    "        drop_last_n (int): jumlah baris terakhir yang dibuang.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame training yang sudah rapi.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pl.read_csv(path)\n",
    "        .rename({\"market_forward_excess_returns\": \"target\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"date_id\").cast(pl.Int32, strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "    if drop_last_n > 0:\n",
    "        df = df.head(-drop_last_n)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_testset(path: Path = TEST_PATH) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load dan praproses test/mock dataset.\n",
    "\n",
    "    - Mengganti nama 'lagged_forward_returns' -> 'target' agar\n",
    "      struktur mirip train (meski tidak dipakai sebagai ground truth).\n",
    "    - Meng-cast semua kolom selain 'date_id' ke Float64.\n",
    "    - Sorting berdasarkan date_id.\n",
    "\n",
    "    Args:\n",
    "        path (Path): lokasi file test.csv.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame test yang sudah rapi.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        pl.read_csv(path)\n",
    "        .rename({\"lagged_forward_returns\": \"target\"})\n",
    "        .with_columns(\n",
    "            pl.col(\"date_id\").cast(pl.Int32, strict=False)\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.exclude(\"date_id\").cast(pl.Float64, strict=False)\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_example_dataset(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Membuat fitur baseline + beberapa fitur tambahan (momentum, trend & volatilitas),\n",
    "    lalu membersihkan DataFrame.\n",
    "\n",
    "    Fitur baru:\n",
    "        - U1 = I2 - I1\n",
    "        - U2 = M11 / mean(I2, I9, I7)\n",
    "        - Untuk setiap fitur dasar F di base_vars:\n",
    "            * F_diff1      : F_t - F_{t-1}         (momentum 1 hari)\n",
    "            * F_rm{w}      : rolling mean w hari   (trend jangka pendek/menengah)\n",
    "            * F_vol{w}     : rolling std w hari    (volatilitas lokal)\n",
    "\n",
    "    Setelah itu:\n",
    "        - Pilih subset kolom: ['date_id', 'target'] + semua fitur di vars_to_keep\n",
    "        - Imputasi missing dengan exponential weighted mean (EWM)\n",
    "        - Drop baris yang masih mengandung null\n",
    "        - Sort by date_id\n",
    "\n",
    "    Args:\n",
    "        df (pl.DataFrame): input Polars DataFrame (train+test gabungan).\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame dengan fitur baru & tanpa null.\n",
    "    \"\"\"\n",
    "    # Fitur dasar yang akan dipakai sebagai anchor untuk FE\n",
    "    base_vars: List[str] = [\n",
    "        \"S2\", \"E2\", \"E3\", \"P9\", \"S1\", \"S5\", \"I2\", \"P8\",\n",
    "        \"P10\", \"P12\", \"P13\",\n",
    "    ]\n",
    "\n",
    "    # Kolom yang wajib ada untuk menghitung U1 & U2\n",
    "    required_base_cols = [\"I1\", \"I2\", \"M11\", \"I7\", \"I9\"] + base_vars\n",
    "    missing = [c for c in required_base_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Kolom berikut hilang di DataFrame: {missing}\")\n",
    "\n",
    "    # Pastikan urutan berdasarkan waktu\n",
    "    df = df.sort(\"date_id\")\n",
    "\n",
    "    # Fitur U1 & U2\n",
    "    df_feat = df.with_columns(\n",
    "        (pl.col(\"I2\") - pl.col(\"I1\")).alias(\"U1\"),\n",
    "        (\n",
    "            pl.col(\"M11\")\n",
    "            / ((pl.col(\"I2\") + pl.col(\"I9\") + pl.col(\"I7\")) / 3.0)\n",
    "        ).alias(\"U2\"),\n",
    "    )\n",
    "\n",
    "    # Konfigurasi horizon untuk trend & volatilitas\n",
    "    trend_windows = [5, 10, 20]   # MA pendek-menengah\n",
    "    vol_windows   = [10, 20]      # volatilitas jangka pendek/menengah\n",
    "\n",
    "    derived_vars: List[str] = []\n",
    "    new_exprs: List[pl.Expr] = []\n",
    "\n",
    "    for col in base_vars:\n",
    "        # Momentum 1 hari\n",
    "        diff_name = f\"{col}_diff1\"\n",
    "        derived_vars.append(diff_name)\n",
    "        new_exprs.append(\n",
    "            (pl.col(col) - pl.col(col).shift(1)).alias(diff_name)\n",
    "        )\n",
    "\n",
    "        # Rolling mean beberapa horizon\n",
    "        for w in trend_windows:\n",
    "            rm_name = f\"{col}_rm{w}\"\n",
    "            derived_vars.append(rm_name)\n",
    "            new_exprs.append(\n",
    "                pl.col(col).rolling_mean(window_size=w, min_periods=1).alias(rm_name)\n",
    "            )\n",
    "\n",
    "        # Rolling std (volatilitas)\n",
    "        for w in vol_windows:\n",
    "            vol_name = f\"{col}_vol{w}\"\n",
    "            derived_vars.append(vol_name)\n",
    "            new_exprs.append(\n",
    "                pl.col(col).rolling_std(window_size=w, min_periods=1).alias(vol_name)\n",
    "            )\n",
    "\n",
    "    # Tambahkan semua fitur turunan dalam satu with_columns supaya efisien\n",
    "    df_feat = df_feat.with_columns(new_exprs)\n",
    "\n",
    "    # Kumpulan semua fitur yang akan kita pakai\n",
    "    vars_to_keep: List[str] = base_vars + derived_vars + [\"U1\", \"U2\"]\n",
    "\n",
    "    # Imputasi missing dengan EWM untuk semua fitur\n",
    "    df_feat = (\n",
    "        df_feat\n",
    "        .select([\"date_id\", \"target\"] + vars_to_keep)\n",
    "        .with_columns(\n",
    "            [\n",
    "                pl.col(col).fill_null(pl.col(col).ewm_mean(com=0.5))\n",
    "                for col in vars_to_keep\n",
    "            ]\n",
    "        )\n",
    "        .drop_nulls()\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "\n",
    "def join_train_test_dataframes(train: pl.DataFrame, test: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Menggabungkan train dan test berdasarkan kolom yang sama\n",
    "    (untuk memastikan feature engineering konsisten).\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): DataFrame training mentah.\n",
    "        test (pl.DataFrame): DataFrame test mentah.\n",
    "\n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame hasil concatenation vertical train+test\n",
    "                      pada kolom-kolom yang sama.\n",
    "    \"\"\"\n",
    "    common_columns: list[str] = [\n",
    "        col for col in train.columns if col in test.columns\n",
    "    ]\n",
    "\n",
    "    if \"date_id\" not in common_columns:\n",
    "        raise KeyError(\"'date_id' harus ada di kedua DataFrame.\")\n",
    "\n",
    "    return (\n",
    "        pl.concat(\n",
    "            [train.select(common_columns), test.select(common_columns)],\n",
    "            how=\"vertical\",\n",
    "        )\n",
    "        .sort(\"date_id\")\n",
    "    )\n",
    "\n",
    "\n",
    "def split_dataset(train: pl.DataFrame, test: pl.DataFrame, features: list[str]) -> DatasetOutput:\n",
    "    \"\"\"\n",
    "    Memisahkan data menjadi fitur (X) dan target (y), lalu melakukan scaling.\n",
    "\n",
    "    Args:\n",
    "        train (pl.DataFrame): DataFrame training yang sudah diproses.\n",
    "        test (pl.DataFrame): DataFrame test yang sudah diproses.\n",
    "        features (list[str]): Daftar nama fitur yang digunakan model.\n",
    "\n",
    "    Returns:\n",
    "        DatasetOutput: Dataclass berisi X_train, y_train, X_test, y_test,\n",
    "                       scaler yang sudah di-fit, feature_names,\n",
    "                       dan date_id masing-masing set.\n",
    "    \"\"\"\n",
    "    # Pastikan kolom wajib ada\n",
    "    for col in [\"date_id\", \"target\"]:\n",
    "        if col not in train.columns or col not in test.columns:\n",
    "            raise KeyError(f\"Kolom wajib '{col}' hilang di train/test.\")\n",
    "\n",
    "    # Simpan date_id untuk keperluan time-series CV nanti\n",
    "    dates_train = train.get_column(\"date_id\")\n",
    "    dates_test  = test.get_column(\"date_id\")\n",
    "\n",
    "    X_train = train.drop([\"date_id\", \"target\"])\n",
    "    y_train = train.get_column(\"target\")\n",
    "\n",
    "    X_test = test.drop([\"date_id\", \"target\"])\n",
    "    y_test = test.get_column(\"target\")\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Sklearn lebih nyaman kalau dikasih numpy array\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_test_np  = X_test.to_numpy()\n",
    "\n",
    "    # fit_transform pada train\n",
    "    X_train_scaled_np = scaler.fit_transform(X_train_np)\n",
    "    X_train_scaled = pl.from_numpy(X_train_scaled_np, schema=features)\n",
    "\n",
    "    # transform pada test\n",
    "    X_test_scaled_np = scaler.transform(X_test_np)\n",
    "    X_test_scaled = pl.from_numpy(X_test_scaled_np, schema=features)\n",
    "\n",
    "    return DatasetOutput(\n",
    "        X_train=X_train_scaled,\n",
    "        X_test=X_test_scaled,\n",
    "        y_train=y_train,\n",
    "        y_test=y_test,\n",
    "        scaler=scaler,\n",
    "        feature_names=features,\n",
    "        dates_train=dates_train,\n",
    "        dates_test=dates_test,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0bb0d",
   "metadata": {
    "papermill": {
     "duration": 0.00534,
     "end_time": "2025-12-04T14:18:00.937701",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.932361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Converting Return Prediction to Signal\n",
    "\n",
    "Here is an example of a potential function used to convert a prediction based on the market forward excess return to a daily signal position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886ab4b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:00.950478Z",
     "iopub.status.busy": "2025-12-04T14:18:00.950107Z",
     "iopub.status.idle": "2025-12-04T14:18:00.963816Z",
     "shell.execute_reply": "2025-12-04T14:18:00.962822Z"
    },
    "papermill": {
     "duration": 0.022255,
     "end_time": "2025-12-04T14:18:00.965577",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.943322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 7. RETURN -> SIGNAL MAPPING\n",
    "# ============================================================\n",
    "\n",
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray | float | list[float],\n",
    "    params: RetToSignalParameters,\n",
    "    debug: bool = False,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert raw model predictions (expected excess returns) into a trading signal.\n",
    "\n",
    "    Mode utama (linear):\n",
    "        signal = clip( ret * signal_multiplier + 1, min_signal, max_signal )\n",
    "\n",
    "    Di mana:\n",
    "        - signal â‰ˆ 1  : posisi netral / benchmark\n",
    "        - signal < 1  : underweight (kurang dari pasar)\n",
    "        - signal > 1  : overweight (lebih agresif dari pasar)\n",
    "\n",
    "    Args:\n",
    "        ret_arr:\n",
    "            Predicted returns (bisa scalar, list, atau numpy array).\n",
    "        params (RetToSignalParameters):\n",
    "            Parameter scaling dan clipping (min/max signal, multiplier).\n",
    "        debug (bool):\n",
    "            Jika True dan bukan mode kompetisi, log ringkas statistik sinyal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            Array sinyal trading dengan shape 1D (n_samples,),\n",
    "            sudah di-clip di [min_signal, max_signal].\n",
    "            (Kalau input scalar, tetap dikembalikan array 1 elemen.)\n",
    "    \"\"\"\n",
    "    # Pastikan dalam bentuk numpy array float (1D)\n",
    "    ret_arr = np.asarray(ret_arr, dtype=float).reshape(-1)\n",
    "\n",
    "    if ret_arr.size == 0:\n",
    "        raise ValueError(\"ret_arr kosong, tidak ada prediksi yang bisa dikonversi ke sinyal.\")\n",
    "\n",
    "    # Sanity-check: tidak boleh ada NaN / inf\n",
    "    if not np.all(np.isfinite(ret_arr)):\n",
    "        raise ValueError(\n",
    "            \"ret_arr mengandung nilai non-finite (NaN/inf). \"\n",
    "            \"Pastikan prediksi model sudah dibersihkan dulu.\"\n",
    "        )\n",
    "\n",
    "    # Mapping linear dari return -> posisi\n",
    "    raw_signal = ret_arr * params.signal_multiplier + 1.0\n",
    "\n",
    "    # Clip supaya tidak keluar dari range yang diizinkan\n",
    "    signal = np.clip(raw_signal, params.min_signal, params.max_signal)\n",
    "\n",
    "    # Debug ringan (hanya kalau diminta, dan bukan di rerun kompetisi)\n",
    "    if debug and not IS_COMP_RERUN:\n",
    "        s_min = float(signal.min())\n",
    "        s_max = float(signal.max())\n",
    "        s_mean = float(signal.mean())\n",
    "        logger.info(\n",
    "            \"[signal debug] n=%d | min=%.4f | max=%.4f | mean=%.4f\",\n",
    "            signal.size, s_min, s_max, s_mean,\n",
    "        )\n",
    "\n",
    "    return signal\n",
    "\n",
    "\n",
    "def convert_ret_to_signal_ranked(\n",
    "    ret_arr: np.ndarray | float | list[float],\n",
    "    params: RetToSignalParameters,\n",
    "    lower_q: float = 0.3,\n",
    "    upper_q: float = 0.7,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Versi alternatif: konversi prediksi return menjadi sinyal berdasarkan ranking.\n",
    "\n",
    "    Ide:\n",
    "      - titik di bawah quantile lower_q  -> bias ke sisi bawah (dekat min_signal)\n",
    "      - titik di atas quantile upper_q  -> bias ke sisi atas (dekat max_signal)\n",
    "      - titik di tengah                 -> sekitar 1.0 (netral)\n",
    "\n",
    "    Cocok untuk eksperimen offline dengan Sharpe internal.\n",
    "    Untuk submission, kamu bisa pilih apakah mau pakai versi linear atau ranking\n",
    "    berdasarkan hasil validasi.\n",
    "\n",
    "    Args:\n",
    "        ret_arr:\n",
    "            Predicted returns (scalar/list/array).\n",
    "        params:\n",
    "            Batas min/max dan titik netral (1.0).\n",
    "        lower_q, upper_q:\n",
    "            Quantile untuk menentukan zona bawah/tengah/atas.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray:\n",
    "            Sinyal 1D terklip dalam [min_signal, max_signal].\n",
    "    \"\"\"\n",
    "    ret_arr = np.asarray(ret_arr, dtype=float).reshape(-1)\n",
    "\n",
    "    if ret_arr.size == 0:\n",
    "        raise ValueError(\"ret_arr kosong, tidak ada prediksi yang bisa dikonversi ke sinyal.\")\n",
    "\n",
    "    if not np.all(np.isfinite(ret_arr)):\n",
    "        raise ValueError(\n",
    "            \"ret_arr mengandung nilai non-finite (NaN/inf). \"\n",
    "            \"Pastikan prediksi model sudah dibersihkan dulu.\"\n",
    "        )\n",
    "\n",
    "    # Hitung threshold quantile\n",
    "    q_low = np.quantile(ret_arr, lower_q)\n",
    "    q_high = np.quantile(ret_arr, upper_q)\n",
    "\n",
    "    # Skala 0â€“1 berdasarkan posisi antara q_low dan q_high\n",
    "    # lalu map ke [min_signal, max_signal], dengan 1.0 sebagai titik netral.\n",
    "    # (Ini bisa kamu tweak lagi setelah lihat hasil Sharpe internal.)\n",
    "    scaled = np.zeros_like(ret_arr, dtype=float)\n",
    "\n",
    "    # Zona bawah\n",
    "    mask_low = ret_arr <= q_low\n",
    "    scaled[mask_low] = params.min_signal\n",
    "\n",
    "    # Zona atas\n",
    "    mask_high = ret_arr >= q_high\n",
    "    scaled[mask_high] = params.max_signal\n",
    "\n",
    "    # Zona tengah: lerp ke sekitar 1.0\n",
    "    mask_mid = ~(mask_low | mask_high)\n",
    "    if mask_mid.any() and q_high > q_low:\n",
    "        mid_vals = ret_arr[mask_mid]\n",
    "        # normalisasi ke [0,1] di antara q_low dan q_high\n",
    "        norm_mid = (mid_vals - q_low) / (q_high - q_low)\n",
    "        # map ke [min_signal, max_signal], tapi tarik ke arah 1.0\n",
    "        mid_signal = params.min_signal + norm_mid * (params.max_signal - params.min_signal)\n",
    "        # blend dengan 1.0 supaya tidak terlalu ekstrem\n",
    "        alpha = 0.5  # 0.5 = setengah ke arah 1.0\n",
    "        mid_signal = alpha * 1.0 + (1 - alpha) * mid_signal\n",
    "        scaled[mask_mid] = mid_signal\n",
    "    else:\n",
    "        # fallback kalau distribusi ret_arr terlalu degenerate\n",
    "        scaled[mask_mid] = 1.0\n",
    "\n",
    "    signal = np.clip(scaled, params.min_signal, params.max_signal)\n",
    "    return signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b69b4",
   "metadata": {
    "papermill": {
     "duration": 0.005412,
     "end_time": "2025-12-04T14:18:00.977373",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.971961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Looking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e659572",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:00.990256Z",
     "iopub.status.busy": "2025-12-04T14:18:00.989956Z",
     "iopub.status.idle": "2025-12-04T14:18:01.539288Z",
     "shell.execute_reply": "2025-12-04T14:18:01.537654Z"
    },
    "papermill": {
     "duration": 0.558802,
     "end_time": "2025-12-04T14:18:01.541854",
     "exception": false,
     "start_time": "2025-12-04T14:18:00.983052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SHAPE ===\n",
      "Train shape: (9011, 98)\n",
      "Test shape : (10, 99)\n",
      "\n",
      "=== DATE RANGE ===\n",
      "Train date_id range: 0 â†’ 9010\n",
      "Test  date_id range: 8980 â†’ 8989\n",
      "\n",
      "=== TRAIN SAMPLE (tail 3) ===\n",
      "shape: (3, 98)\n",
      "â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\n",
      "â”‚ dat â”† D1  â”† D2  â”† D3  â”† D4  â”† D5  â”† D6  â”† D7  â”† D8  â”† D9  â”† â€¦ â”† V3  â”† V4  â”† V5  â”† V6  â”† V7  â”† V8  â”† V9  â”† for â”† ris â”† tar â”‚\n",
      "â”‚ e_i â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”†   â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”† --- â”† war â”† k_f â”† get â”‚\n",
      "â”‚ d   â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”†   â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† f64 â”† d_r â”† ree â”† --- â”‚\n",
      "â”‚ --- â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”† etu â”† _ra â”† f64 â”‚\n",
      "â”‚ i32 â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”† rns â”† te  â”†     â”‚\n",
      "â”‚     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”† --- â”† --- â”†     â”‚\n",
      "â”‚     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”† f64 â”† f64 â”†     â”‚\n",
      "â•žâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•¡\n",
      "â”‚ 900 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† â€¦ â”† 0.8 â”† 0.2 â”† 0.5 â”† 0.4 â”† -0. â”† 0.2 â”† -0. â”† -0. â”† 0.0 â”† -0. â”‚\n",
      "â”‚ 8   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”† 359 â”† 843 â”† 384 â”† 378 â”† 567 â”† 218 â”† 530 â”† 002 â”† 001 â”† 003 â”‚\n",
      "â”‚     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”† 79  â”† 92  â”† 83  â”† 31  â”† 113 â”† 92  â”† 228 â”† 897 â”† 525 â”† 362 â”‚\n",
      "â”‚ 900 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† â€¦ â”† 0.3 â”† 0.2 â”† 0.5 â”† 0.5 â”† -0. â”† 0.1 â”† -0. â”† -0. â”† 0.0 â”† -0. â”‚\n",
      "â”‚ 9   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”† 366 â”† 943 â”† 790 â”† 972 â”† 550 â”† 392 â”† 512 â”† 027 â”† 001 â”† 027 â”‚\n",
      "â”‚     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”† 4   â”† 12  â”† 49  â”† 22  â”† 57  â”† 2   â”† 769 â”† 028 â”† 53  â”† 493 â”‚\n",
      "â”‚ 901 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† 0.0 â”† â€¦ â”† 0.4 â”† 0.2 â”† 0.5 â”† 0.7 â”† 0.1 â”† 0.1 â”† -0. â”† 0.0 â”† 0.0 â”† 0.0 â”‚\n",
      "â”‚ 0   â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”† 761 â”† 962 â”† 456 â”† 519 â”† 036 â”† 547 â”† 015 â”† 153 â”† 001 â”† 148 â”‚\n",
      "â”‚     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†     â”†   â”† 9   â”† 96  â”† 66  â”† 84  â”† 92  â”† 62  â”† 503 â”† 44  â”† 53  â”† 79  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "=== TEST SAMPLE (head 5) ===\n",
      "shape: (5, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ date_id â”† is_scored â”† target    â”‚\n",
      "â”‚ ---     â”† ---       â”† ---       â”‚\n",
      "â”‚ i32     â”† f64       â”† f64       â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ 8980    â”† 1.0       â”† 0.003541  â”‚\n",
      "â”‚ 8981    â”† 1.0       â”† -0.005964 â”‚\n",
      "â”‚ 8982    â”† 1.0       â”† -0.00741  â”‚\n",
      "â”‚ 8983    â”† 1.0       â”† 0.00542   â”‚\n",
      "â”‚ 8984    â”† 1.0       â”† 0.008357  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "=== TARGET STATS (train.target) ===\n",
      "shape: (9, 2)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ statistic  â”† target    â”‚\n",
      "â”‚ ---        â”† ---       â”‚\n",
      "â”‚ str        â”† f64       â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ count      â”† 9011.0    â”‚\n",
      "â”‚ null_count â”† 0.0       â”‚\n",
      "â”‚ mean       â”† 0.00005   â”‚\n",
      "â”‚ std        â”† 0.010562  â”‚\n",
      "â”‚ min        â”† -0.040582 â”‚\n",
      "â”‚ 25%        â”† -0.004747 â”‚\n",
      "â”‚ 50%        â”† 0.000253  â”‚\n",
      "â”‚ 75%        â”† 0.005479  â”‚\n",
      "â”‚ max        â”† 0.040551  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "=== TARGET SIGN COUNTS (train.target) ===\n",
      "n       : 9011\n",
      "> 0     : 51.670%\n",
      "< 0     : 48.330%\n",
      "== 0    : 0.000%\n",
      "\n",
      "=== TARGET QUANTILES (train.target) ===\n",
      "5%      : -0.018087\n",
      "95%     : 0.016418\n",
      "max|x|  : 0.040582\n",
      "\n",
      "=== TOP 10 NULL COUNTS (TRAIN) ===\n",
      "shape: (10, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ column â”† n_null â”† null_frac â”‚\n",
      "â”‚ ---    â”† ---    â”† ---       â”‚\n",
      "â”‚ str    â”† u32    â”† f64       â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ E7     â”† 6969   â”† 0.773388  â”‚\n",
      "â”‚ V10    â”† 6049   â”† 0.671291  â”‚\n",
      "â”‚ S3     â”† 5733   â”† 0.636222  â”‚\n",
      "â”‚ M1     â”† 5547   â”† 0.615581  â”‚\n",
      "â”‚ M13    â”† 5540   â”† 0.614804  â”‚\n",
      "â”‚ M14    â”† 5540   â”† 0.614804  â”‚\n",
      "â”‚ M6     â”† 5043   â”† 0.559649  â”‚\n",
      "â”‚ V9     â”† 4539   â”† 0.503718  â”‚\n",
      "â”‚ S12    â”† 3537   â”† 0.39252   â”‚\n",
      "â”‚ M5     â”† 3283   â”† 0.364332  â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "=== TOP 10 NULL COUNTS (TEST) ===\n",
      "shape: (10, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ column  â”† n_null â”† null_frac â”‚\n",
      "â”‚ ---     â”† ---    â”† ---       â”‚\n",
      "â”‚ str     â”† u32    â”† f64       â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ date_id â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D1      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D2      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D3      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D4      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D5      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D6      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D7      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D8      â”† 0      â”† 0.0       â”‚\n",
      "â”‚ D9      â”† 0      â”† 0.0       â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "=== TOP 15 |CORR(feature, target)| (TRAIN) ===\n",
      "shape: (15, 3)\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ feature         â”† corr_target â”† abs_corr â”‚\n",
      "â”‚ ---             â”† ---         â”† ---      â”‚\n",
      "â”‚ str             â”† f64         â”† f64      â”‚\n",
      "â•žâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
      "â”‚ forward_returns â”† 0.999943    â”† 0.999943 â”‚\n",
      "â”‚ M4              â”† -0.066655   â”† 0.066655 â”‚\n",
      "â”‚ V13             â”† 0.062442    â”† 0.062442 â”‚\n",
      "â”‚ M1              â”† 0.04649     â”† 0.04649  â”‚\n",
      "â”‚ S5              â”† 0.04012     â”† 0.04012  â”‚\n",
      "â”‚ S2              â”† -0.037852   â”† 0.037852 â”‚\n",
      "â”‚ D1              â”† 0.034003    â”† 0.034003 â”‚\n",
      "â”‚ D2              â”† 0.034003    â”† 0.034003 â”‚\n",
      "â”‚ M2              â”† 0.033221    â”† 0.033221 â”‚\n",
      "â”‚ V10             â”† 0.032827    â”† 0.032827 â”‚\n",
      "â”‚ E7              â”† -0.032406   â”† 0.032406 â”‚\n",
      "â”‚ E11             â”† -0.031995   â”† 0.031995 â”‚\n",
      "â”‚ V7              â”† 0.031566    â”† 0.031566 â”‚\n",
      "â”‚ E12             â”† -0.030801   â”† 0.030801 â”‚\n",
      "â”‚ P8              â”† -0.029734   â”† 0.029734 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3078380767.py:79: DeprecationWarning: `DataFrame.melt` is deprecated. Use `unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  .melt(variable_name=\"column\", value_name=\"n_null\")\n",
      "/tmp/ipykernel_13/3078380767.py:91: DeprecationWarning: `DataFrame.melt` is deprecated. Use `unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  .melt(variable_name=\"column\", value_name=\"n_null\")\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. QUICK DATA CHECK: TRAIN & TEST (LOCAL ONLY)\n",
    "# ============================================================\n",
    "\n",
    "# Bagian ini hanya dijalankan saat TIDAK rerun kompetisi,\n",
    "# supaya tidak menambah waktu eksekusi saat submit.\n",
    "if not IS_COMP_RERUN:\n",
    "    # --------------------------------------------------------\n",
    "    # 8.1 Load data mentah\n",
    "    # --------------------------------------------------------\n",
    "    train: pl.DataFrame = load_trainset()\n",
    "    test: pl.DataFrame  = load_testset()\n",
    "\n",
    "    print(\"=== SHAPE ===\")\n",
    "    print(\"Train shape:\", train.shape)\n",
    "    print(\"Test shape :\", test.shape)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 8.2 Range date_id untuk memastikan urut dan tidak bolong\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n=== DATE RANGE ===\")\n",
    "    print(\n",
    "        \"Train date_id range:\",\n",
    "        int(train[\"date_id\"].min()),\n",
    "        \"â†’\",\n",
    "        int(train[\"date_id\"].max()),\n",
    "    )\n",
    "    print(\n",
    "        \"Test  date_id range:\",\n",
    "        int(test[\"date_id\"].min()),\n",
    "        \"â†’\",\n",
    "        int(test[\"date_id\"].max()),\n",
    "    )\n",
    "\n",
    "    # Cek beberapa baris terakhir train\n",
    "    print(\"\\n=== TRAIN SAMPLE (tail 3) ===\")\n",
    "    print(train.tail(3))\n",
    "\n",
    "    # Cek beberapa kolom penting di test (date_id, is_scored, target)\n",
    "    cols_to_show_test = [\n",
    "        c for c in [\"date_id\", \"is_scored\", \"target\"] if c in test.columns\n",
    "    ]\n",
    "    print(\"\\n=== TEST SAMPLE (head 5) ===\")\n",
    "    print(test.select(cols_to_show_test).head(5))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 8.3 Statistik dasar target\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n=== TARGET STATS (train.target) ===\")\n",
    "    print(train.select(\"target\").describe())\n",
    "\n",
    "    # Tambahan: proporsi return positif/negatif & outlier kasar\n",
    "    target_np = train[\"target\"].to_numpy()\n",
    "    n = target_np.size\n",
    "    pos_ratio = float((target_np > 0).sum()) / n\n",
    "    neg_ratio = float((target_np < 0).sum()) / n\n",
    "    zero_ratio = float((target_np == 0).sum()) / n\n",
    "\n",
    "    print(\"\\n=== TARGET SIGN COUNTS (train.target) ===\")\n",
    "    print(f\"n       : {n}\")\n",
    "    print(f\"> 0     : {pos_ratio:.3%}\")\n",
    "    print(f\"< 0     : {neg_ratio:.3%}\")\n",
    "    print(f\"== 0    : {zero_ratio:.3%}\")\n",
    "\n",
    "    # Outlier sederhana (5th, 95th, max abs)\n",
    "    q05, q95 = np.quantile(target_np, [0.05, 0.95])\n",
    "    max_abs = float(np.max(np.abs(target_np)))\n",
    "    print(\"\\n=== TARGET QUANTILES (train.target) ===\")\n",
    "    print(f\"5%      : {q05:.6f}\")\n",
    "    print(f\"95%     : {q95:.6f}\")\n",
    "    print(f\"max|x|  : {max_abs:.6f}\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 8.4 Missing values (top 10)\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n=== TOP 10 NULL COUNTS (TRAIN) ===\")\n",
    "    null_counts_train = (\n",
    "        train.null_count()\n",
    "        .melt(variable_name=\"column\", value_name=\"n_null\")\n",
    "        .with_columns(\n",
    "            (pl.col(\"n_null\") / train.height).alias(\"null_frac\")\n",
    "        )\n",
    "        .sort(\"n_null\", descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(null_counts_train)\n",
    "\n",
    "    print(\"\\n=== TOP 10 NULL COUNTS (TEST) ===\")\n",
    "    null_counts_test = (\n",
    "        test.null_count()\n",
    "        .melt(variable_name=\"column\", value_name=\"n_null\")\n",
    "        .with_columns(\n",
    "            (pl.col(\"n_null\") / test.height).alias(\"null_frac\")\n",
    "        )\n",
    "        .sort(\"n_null\", descending=True)\n",
    "        .head(10)\n",
    "    )\n",
    "    print(null_counts_test)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 8.5 Korelasi sederhana fiturâ€“target (top 15)\n",
    "    # --------------------------------------------------------\n",
    "    # Hanya untuk numeric, dan exclude date_id/target\n",
    "    num_cols = [\n",
    "        c for c, dt in train.schema.items()\n",
    "        if c not in (\"date_id\", \"target\") and dt in (pl.Float64, pl.Float32, pl.Int64, pl.Int32)\n",
    "    ]\n",
    "\n",
    "    if num_cols:\n",
    "        # pakai pandas sebentar untuk corrwith (lebih simpel)\n",
    "        train_pd = train.select([\"target\"] + num_cols).to_pandas()\n",
    "        corr_series = train_pd[num_cols].corrwith(train_pd[\"target\"])\n",
    "        corr_df = (\n",
    "            pl.DataFrame({\n",
    "                \"feature\": corr_series.index.to_list(),\n",
    "                \"corr_target\": corr_series.values,\n",
    "            })\n",
    "            .with_columns(\n",
    "                pl.col(\"corr_target\").abs().alias(\"abs_corr\")\n",
    "            )\n",
    "            .sort(\"abs_corr\", descending=True)\n",
    "            .head(15)\n",
    "        )\n",
    "\n",
    "        print(\"\\n=== TOP 15 |CORR(feature, target)| (TRAIN) ===\")\n",
    "        print(corr_df)\n",
    "    else:\n",
    "        print(\"\\n[WARN] Tidak ada numeric feature yang bisa dihitung korelasinya.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8c4a5",
   "metadata": {
    "papermill": {
     "duration": 0.005766,
     "end_time": "2025-12-04T14:18:01.554759",
     "exception": false,
     "start_time": "2025-12-04T14:18:01.548993",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generating the Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ad8ad3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:01.569218Z",
     "iopub.status.busy": "2025-12-04T14:18:01.568849Z",
     "iopub.status.idle": "2025-12-04T14:18:01.758048Z",
     "shell.execute_reply": "2025-12-04T14:18:01.756645Z"
    },
    "papermill": {
     "duration": 0.199084,
     "end_time": "2025-12-04T14:18:01.759824",
     "exception": false,
     "start_time": "2025-12-04T14:18:01.560740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] df_fe height     : 7509\n",
      "[DEBUG] train_fe + test_fe: 7529\n",
      "[WARN] Ada baris yang tidak terklasifikasi ke train/test berdasarkan date_id.\n",
      "=== FE & SPLIT SUMMARY ===\n",
      "X_train shape : (7509, 79)\n",
      "X_test  shape : (20, 79)\n",
      "y_train length: 7509\n",
      "y_test  length: 20\n",
      "Num features  : 79\n",
      "Train dates   : 1512 â†’ 9010\n",
      "Test  dates   : 8980 â†’ 8989\n",
      "Test is_scored=1 count : 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3594239916.py:139: DeprecationWarning: The argument `min_periods` for `Expr.rolling_mean` is deprecated. It has been renamed to `min_samples`.\n",
      "  pl.col(col).rolling_mean(window_size=w, min_periods=1).alias(rm_name)\n",
      "/tmp/ipykernel_13/3594239916.py:147: DeprecationWarning: The argument `min_periods` for `Expr.rolling_std` is deprecated. It has been renamed to `min_samples`.\n",
      "  pl.col(col).rolling_std(window_size=w, min_periods=1).alias(vol_name)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. FEATURE ENGINEERING + TRAIN/TEST SPLIT\n",
    "# ============================================================\n",
    "\n",
    "# 1) Selalu load train & test mentah di sini\n",
    "#    (jangan bergantung ke cell EDA yang hanya jalan saat not IS_COMP_RERUN)\n",
    "train_raw: pl.DataFrame = load_trainset()\n",
    "test_raw: pl.DataFrame  = load_testset()\n",
    "\n",
    "# (Opsional) Simpan is_scored dari test untuk analisis lokal / LB-mirroring\n",
    "is_scored_test: Optional[pl.Series] = (\n",
    "    test_raw.get_column(\"is_scored\") if \"is_scored\" in test_raw.columns else None\n",
    ")\n",
    "\n",
    "# 2) Gabungkan train & test mentah pada kolom yang sama (supaya FE konsisten)\n",
    "df_all: pl.DataFrame = join_train_test_dataframes(train_raw, test_raw)\n",
    "\n",
    "# 3) Terapkan feature engineering (U1, U2, momentum, rolling trend & vol, EWM fill, drop_null)\n",
    "df_fe: pl.DataFrame = create_example_dataset(df=df_all)\n",
    "\n",
    "# 4) Kembalikan lagi ke train_fe dan test_fe berdasarkan date_id awal\n",
    "train_ids = train_raw.get_column(\"date_id\").unique()\n",
    "test_ids  = test_raw.get_column(\"date_id\").unique()\n",
    "\n",
    "train_fe: pl.DataFrame = df_fe.filter(pl.col(\"date_id\").is_in(train_ids))\n",
    "test_fe: pl.DataFrame  = df_fe.filter(pl.col(\"date_id\").is_in(test_ids))\n",
    "\n",
    "# Pastikan sudah terurut waktu\n",
    "train_fe = train_fe.sort(\"date_id\")\n",
    "test_fe  = test_fe.sort(\"date_id\")\n",
    "\n",
    "# Sanity-check: tidak ada overlap aneh (harusnya union = seluruh df_fe)\n",
    "if not IS_COMP_RERUN:\n",
    "    n_all = df_fe.height\n",
    "    n_union = train_fe.height + test_fe.height\n",
    "    print(f\"[DEBUG] df_fe height     : {n_all}\")\n",
    "    print(f\"[DEBUG] train_fe + test_fe: {n_union}\")\n",
    "    if n_union != n_all:\n",
    "        print(\"[WARN] Ada baris yang tidak terklasifikasi ke train/test berdasarkan date_id.\")\n",
    "\n",
    "# 5) Definisikan daftar fitur (semua kolom kecuali 'date_id' dan 'target')\n",
    "FEATURES: list[str] = sorted(\n",
    "    [col for col in test_fe.columns if col not in [\"date_id\", \"target\"]]\n",
    ")\n",
    "\n",
    "# 6) Split menjadi X/y + scaling, dibungkus dalam DatasetOutput\n",
    "dataset: DatasetOutput = split_dataset(\n",
    "    train=train_fe,\n",
    "    test=test_fe,\n",
    "    features=FEATURES,\n",
    ")\n",
    "\n",
    "X_train: pl.DataFrame      = dataset.X_train\n",
    "X_test: pl.DataFrame       = dataset.X_test\n",
    "y_train: pl.Series         = dataset.y_train\n",
    "y_test: pl.Series          = dataset.y_test\n",
    "scaler: StandardScaler     = dataset.scaler\n",
    "feature_names: list[str]   = dataset.feature_names or FEATURES\n",
    "dates_train: pl.Series     = dataset.dates_train\n",
    "dates_test: pl.Series      = dataset.dates_test\n",
    "\n",
    "# (Opsional) Quick check â€“ hanya saat lokal, supaya tidak nambah waktu di kompetisi\n",
    "if not IS_COMP_RERUN:\n",
    "    print(\"=== FE & SPLIT SUMMARY ===\")\n",
    "    print(\"X_train shape :\", X_train.shape)\n",
    "    print(\"X_test  shape :\", X_test.shape)\n",
    "    print(\"y_train length:\", y_train.len())\n",
    "    print(\"y_test  length:\", y_test.len())\n",
    "    print(\"Num features  :\", len(FEATURES))\n",
    "    print(\"Train dates   :\", int(dates_train.min()), \"â†’\", int(dates_train.max()))\n",
    "    print(\"Test  dates   :\", int(dates_test.min()),  \"â†’\", int(dates_test.max()))\n",
    "\n",
    "    assert X_train.height == y_train.len(), \"X_train dan y_train tidak sebaris!\"\n",
    "    assert X_test.height == y_test.len(), \"X_test dan y_test tidak sebaris!\"\n",
    "\n",
    "    if is_scored_test is not None:\n",
    "        n_scored = int((is_scored_test == 1).sum())\n",
    "        print(f\"Test is_scored=1 count : {n_scored} / {is_scored_test.len()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c950d82d",
   "metadata": {
    "papermill": {
     "duration": 0.007084,
     "end_time": "2025-12-04T14:18:01.774569",
     "exception": false,
     "start_time": "2025-12-04T14:18:01.767485",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fitting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee8326b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:01.789457Z",
     "iopub.status.busy": "2025-12-04T14:18:01.789104Z",
     "iopub.status.idle": "2025-12-04T14:18:11.125740Z",
     "shell.execute_reply": "2025-12-04T14:18:11.124882Z"
    },
    "papermill": {
     "duration": 9.34613,
     "end_time": "2025-12-04T14:18:11.127517",
     "exception": false,
     "start_time": "2025-12-04T14:18:01.781387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TS-CV] Built 5 folds (n_folds target = 5)\n",
      "  Fold 0: train 1512â†’2757 | val 2761â†’4009 | n_train=1246, n_val=1249\n",
      "  Fold 1: train 1512â†’4006 | val 4010â†’5258 | n_train=2495, n_val=1249\n",
      "  Fold 2: train 1512â†’5255 | val 5259â†’6507 | n_train=3744, n_val=1249\n",
      "  Fold 3: train 1512â†’6504 | val 6508â†’7756 | n_train=4993, n_val=1249\n",
      "  Fold 4: train 1512â†’7753 | val 7757â†’9010 | n_train=6242, n_val=1264\n",
      "\n",
      "[TS-CV] Top 5 alpha berdasarkan Sharpe (annualised):\n",
      "  alpha=0.000266 | Sharpe=0.3673\n",
      "  alpha=0.000305 | Sharpe=0.3667\n",
      "  alpha=0.000351 | Sharpe=0.3658\n",
      "  alpha=0.000231 | Sharpe=0.3657\n",
      "  alpha=0.000404 | Sharpe=0.3610\n",
      "\n",
      "[TS-CV] Best alpha (by Sharpe) : 0.000266\n",
      "[TS-CV] Best Sharpe (mean CV)  : 0.3673\n",
      "\n",
      "=== ElasticNet Baseline Fitted (TS-CV Sharpe-tuned) ===\n",
      "Best alpha (Sharpe-CV): 0.000266\n",
      "L1 ratio              : 0.5\n",
      "Train RÂ²              : 0.014995\n",
      "Train MSE             : 1.212204e-04\n",
      "(Internal CV Sharpe   : 0.3673 annualised)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. FIT ELASTICNET BASELINE (TIME-SERIES CV + SHARPE)\n",
    "# ============================================================\n",
    "\n",
    "# Sklearn lebih aman kalau dikasih numpy array\n",
    "X_train_np = X_train.to_numpy()\n",
    "y_train_np = y_train.to_numpy()\n",
    "dates_train_np = dates_train.to_numpy()\n",
    "\n",
    "def compute_sharpe(\n",
    "    y_true: np.ndarray,\n",
    "    y_pred: np.ndarray,\n",
    "    signal_params: RetToSignalParameters,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Hitung Sharpe ratio tahunan sederhana dari prediksi return.\n",
    "\n",
    "    - Konversi prediksi return -> sinyal (0..2) via convert_ret_to_signal\n",
    "    - Return strategi harian = signal * y_true\n",
    "    - Sharpe harian = mean / std\n",
    "    - Sharpe tahunan = Sharpe harian * sqrt(252)\n",
    "    \"\"\"\n",
    "    signal = convert_ret_to_signal(y_pred, signal_params, debug=False)\n",
    "    strat_ret = signal * y_true\n",
    "\n",
    "    vol = strat_ret.std(ddof=1)\n",
    "    if vol == 0 or not np.isfinite(vol):\n",
    "        return 0.0\n",
    "\n",
    "    mean_ret = strat_ret.mean()\n",
    "    sharpe_daily = mean_ret / vol\n",
    "    sharpe_annual = float(sharpe_daily * EVAL_CFG.sharpe_annualization_factor)\n",
    "    return sharpe_annual\n",
    "\n",
    "\n",
    "def build_time_series_folds(\n",
    "    dates: np.ndarray,\n",
    "    n_folds: int,\n",
    "    gap_days: int = 3,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Bangun fold time-series expanding window sederhana berbasis date_id.\n",
    "\n",
    "    - Setiap fold:\n",
    "        * train: semua tanggal sebelum window val, dikurangi gap_days terakhir\n",
    "        * val  : satu window tanggal setelah train\n",
    "    \"\"\"\n",
    "    unique_dates = np.unique(dates)\n",
    "    unique_dates.sort()\n",
    "\n",
    "    n_dates = len(unique_dates)\n",
    "    if n_dates < (n_folds + 1):\n",
    "        raise ValueError(f\"Jumlah tanggal ({n_dates}) terlalu sedikit untuk n_folds={n_folds}.\")\n",
    "\n",
    "    fold_size = n_dates // (n_folds + 1)  # 1 blok awal + n_folds blok val\n",
    "\n",
    "    folds: list[dict] = []\n",
    "    for k in range(n_folds):\n",
    "        # Definisi window tanggal untuk fold k\n",
    "        val_start_idx = (k + 1) * fold_size\n",
    "        val_end_idx   = (k + 2) * fold_size if k < n_folds - 1 else n_dates\n",
    "\n",
    "        train_end_idx = max(0, val_start_idx - gap_days)\n",
    "        train_dates = unique_dates[:train_end_idx]\n",
    "        val_dates   = unique_dates[val_start_idx:val_end_idx]\n",
    "\n",
    "        train_mask = np.isin(dates, train_dates)\n",
    "        val_mask   = np.isin(dates, val_dates)\n",
    "\n",
    "        train_idx = np.where(train_mask)[0]\n",
    "        val_idx   = np.where(val_mask)[0]\n",
    "\n",
    "        if train_idx.size == 0 or val_idx.size == 0:\n",
    "            continue\n",
    "\n",
    "        folds.append(\n",
    "            {\n",
    "                \"fold_idx\": k,\n",
    "                \"train_idx\": train_idx,\n",
    "                \"val_idx\": val_idx,\n",
    "                \"train_start_date\": int(train_dates.min()) if train_dates.size > 0 else int(unique_dates[0]),\n",
    "                \"train_end_date\": int(train_dates.max()) if train_dates.size > 0 else int(unique_dates[0]),\n",
    "                \"val_start_date\": int(val_dates.min()),\n",
    "                \"val_end_date\": int(val_dates.max()),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not folds:\n",
    "        raise RuntimeError(\"Gagal membentuk time-series folds. Cek konfigurasi n_folds / gap_days.\")\n",
    "\n",
    "    return folds\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10.1 Bangun time-series folds\n",
    "# ------------------------------------------------------------\n",
    "ts_folds = build_time_series_folds(\n",
    "    dates=dates_train_np,\n",
    "    n_folds=EVAL_CFG.n_folds,\n",
    "    gap_days=3,\n",
    ")\n",
    "\n",
    "if not IS_COMP_RERUN:\n",
    "    print(f\"[TS-CV] Built {len(ts_folds)} folds (n_folds target = {EVAL_CFG.n_folds})\")\n",
    "    for f in ts_folds:\n",
    "        print(\n",
    "            f\"  Fold {f['fold_idx']}: \"\n",
    "            f\"train {f['train_start_date']}â†’{f['train_end_date']} \"\n",
    "            f\"| val {f['val_start_date']}â†’{f['val_end_date']} \"\n",
    "            f\"| n_train={len(f['train_idx'])}, n_val={len(f['val_idx'])}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10.2 Grid search alpha berbasis Sharpe internal\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "candidate_alphas = enet_params.alphas  # dari konfigurasi global\n",
    "alpha_sharpe_scores: dict[float, float] = {}\n",
    "\n",
    "for alpha in candidate_alphas:\n",
    "    fold_sharpes: list[float] = []\n",
    "\n",
    "    for f in ts_folds:\n",
    "        tr_idx = f[\"train_idx\"]\n",
    "        va_idx = f[\"val_idx\"]\n",
    "\n",
    "        X_tr, y_tr = X_train_np[tr_idx], y_train_np[tr_idx]\n",
    "        X_va, y_va = X_train_np[va_idx], y_train_np[va_idx]\n",
    "\n",
    "        # Model per-fold (ElasticNet)\n",
    "        model_fold = ElasticNet(\n",
    "            alpha=float(alpha),\n",
    "            l1_ratio=enet_params.l1_ratio,\n",
    "            max_iter=enet_params.max_iter,\n",
    "            fit_intercept=enet_params.fit_intercept,\n",
    "            random_state=enet_params.random_state,\n",
    "        )\n",
    "        model_fold.fit(X_tr, y_tr)\n",
    "\n",
    "        y_va_pred = model_fold.predict(X_va)\n",
    "        sharpe_val = compute_sharpe(y_true=y_va, y_pred=y_va_pred, signal_params=ret_signal_params)\n",
    "        fold_sharpes.append(sharpe_val)\n",
    "\n",
    "    if fold_sharpes:\n",
    "        alpha_sharpe_scores[float(alpha)] = float(np.mean(fold_sharpes))\n",
    "\n",
    "# Pilih alpha dengan Sharpe rata-rata tertinggi\n",
    "if not alpha_sharpe_scores:\n",
    "    raise RuntimeError(\"Tidak ada skor Sharpe yang berhasil dihitung dalam TS-CV.\")\n",
    "\n",
    "best_alpha = max(alpha_sharpe_scores, key=alpha_sharpe_scores.get)\n",
    "best_sharpe = alpha_sharpe_scores[best_alpha]\n",
    "\n",
    "# (Opsional) lihat beberapa alpha terbaik saat lokal\n",
    "if not IS_COMP_RERUN:\n",
    "    sorted_alphas = sorted(alpha_sharpe_scores.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    print(\"\\n[TS-CV] Top 5 alpha berdasarkan Sharpe (annualised):\")\n",
    "    for a, s in sorted_alphas[:5]:\n",
    "        print(f\"  alpha={a:.6f} | Sharpe={s:.4f}\")\n",
    "\n",
    "    print(f\"\\n[TS-CV] Best alpha (by Sharpe) : {best_alpha:.6f}\")\n",
    "    print(f\"[TS-CV] Best Sharpe (mean CV)  : {best_sharpe:.4f}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 10.3 Fit final ElasticNet di seluruh training data\n",
    "# ------------------------------------------------------------\n",
    "model: ElasticNet = ElasticNet(\n",
    "    alpha=best_alpha,\n",
    "    l1_ratio=enet_params.l1_ratio,\n",
    "    max_iter=enet_params.max_iter,\n",
    "    fit_intercept=enet_params.fit_intercept,\n",
    "    random_state=enet_params.random_state,\n",
    ")\n",
    "model.fit(X_train_np, y_train_np)\n",
    "\n",
    "# Quick sanity check: performa di training (bukan metric kompetisi, hanya cek leak/bug)\n",
    "y_pred_train = model.predict(X_train_np)\n",
    "r2_train = r2_score(y_train_np, y_pred_train)\n",
    "mse_train = mean_squared_error(y_train_np, y_pred_train)\n",
    "\n",
    "print(\"\\n=== ElasticNet Baseline Fitted (TS-CV Sharpe-tuned) ===\")\n",
    "print(f\"Best alpha (Sharpe-CV): {best_alpha:.6f}\")\n",
    "print(f\"L1 ratio              : {enet_params.l1_ratio}\")\n",
    "print(f\"Train RÂ²              : {r2_train:.6f}\")\n",
    "print(f\"Train MSE             : {mse_train:.6e}\")\n",
    "print(f\"(Internal CV Sharpe   : {best_sharpe:.4f} annualised)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c54ca70",
   "metadata": {
    "papermill": {
     "duration": 0.009607,
     "end_time": "2025-12-04T14:18:11.147685",
     "exception": false,
     "start_time": "2025-12-04T14:18:11.138078",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction Function via Kaggle Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b4f006f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:11.169249Z",
     "iopub.status.busy": "2025-12-04T14:18:11.168446Z",
     "iopub.status.idle": "2025-12-04T14:18:11.179972Z",
     "shell.execute_reply": "2025-12-04T14:18:11.179194Z"
    },
    "papermill": {
     "duration": 0.024221,
     "end_time": "2025-12-04T14:18:11.181791",
     "exception": false,
     "start_time": "2025-12-04T14:18:11.157570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 11. PREDICTION FUNCTION FOR KAGGLE EVALUATION API\n",
    "# ============================================================\n",
    "\n",
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Fungsi prediksi yang dipanggil oleh Kaggle Evaluation API.\n",
    "\n",
    "    Alur:\n",
    "    1) Rename kolom target (lagged_forward_returns -> target) jika perlu.\n",
    "    2) Terapkan feature engineering (create_example_dataset).\n",
    "    3) Pilih fitur sesuai FEATURES dan lakukan scaling dengan scaler yang sudah di-fit.\n",
    "    4) Prediksi expected excess return pakai model.\n",
    "    5) Konversi return -> sinyal trading via convert_ret_to_signal.\n",
    "    6) Return satu nilai float (sinyal) untuk batch pertama.\n",
    "\n",
    "    Catatan:\n",
    "    - DefaultInferenceServer biasanya memanggil fungsi ini dengan\n",
    "      1 baris per call, tapi kita tetap handle jika >1 baris.\n",
    "    \"\"\"\n",
    "    # 1. Pastikan kolom target ada\n",
    "    if \"target\" not in test.columns:\n",
    "        if \"lagged_forward_returns\" in test.columns:\n",
    "            test = test.rename({\"lagged_forward_returns\": \"target\"})\n",
    "        else:\n",
    "            raise KeyError(\n",
    "                \"Test dataframe harus memiliki kolom 'target' atau 'lagged_forward_returns'.\"\n",
    "            )\n",
    "\n",
    "    # 2. Pastikan 'date_id' ada\n",
    "    if \"date_id\" not in test.columns:\n",
    "        raise KeyError(\"Kolom 'date_id' wajib ada di dataframe test.\")\n",
    "\n",
    "    # 3. Feature engineering (U1, U2, imputasi EWM, subset fitur, drop_null)\n",
    "    df = create_example_dataset(test)\n",
    "\n",
    "    # Jika setelah FE tidak ada baris (misal semua null dan ter-drop),\n",
    "    # kembalikan sinyal netral 1.0 agar tidak crash saat evaluasi.\n",
    "    if df.height == 0:\n",
    "        return float(1.0)\n",
    "\n",
    "    # 4. Ambil hanya fitur yang sudah didefinisikan di training\n",
    "    X_test = df.select(FEATURES)\n",
    "\n",
    "    # Sanity-check: pastikan semua fitur ada\n",
    "    missing_feats = [f for f in FEATURES if f not in X_test.columns]\n",
    "    if missing_feats:\n",
    "        raise KeyError(\n",
    "            f\"Fitur berikut hilang di data FE test: {missing_feats}\"\n",
    "        )\n",
    "\n",
    "    # 5. Sklearn pakai numpy array\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    X_test_scaled_np = scaler.transform(X_test_np)\n",
    "\n",
    "    # 6. Prediksi expected excess return\n",
    "    raw_pred = model.predict(X_test_scaled_np)  # array shape (n_samples,)\n",
    "\n",
    "    # 7. Konversi ke sinyal trading\n",
    "    signal_arr = convert_ret_to_signal(raw_pred, ret_signal_params)\n",
    "\n",
    "    # 8. Ambil satu nilai (biasanya satu baris per call)\n",
    "    return float(signal_arr[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d14043",
   "metadata": {
    "papermill": {
     "duration": 0.009798,
     "end_time": "2025-12-04T14:18:11.201442",
     "exception": false,
     "start_time": "2025-12-04T14:18:11.191644",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Launch Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c206420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T14:18:11.222982Z",
     "iopub.status.busy": "2025-12-04T14:18:11.222614Z",
     "iopub.status.idle": "2025-12-04T14:18:11.533125Z",
     "shell.execute_reply": "2025-12-04T14:18:11.532138Z"
    },
    "papermill": {
     "duration": 0.323434,
     "end_time": "2025-12-04T14:18:11.534906",
     "exception": false,
     "start_time": "2025-12-04T14:18:11.211472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in LOCAL GATEWAY mode for debugging...\n",
      "Using input dir: /kaggle/input/hull-tactical-market-prediction\n",
      "\n",
      "submission.parquet generated at: /kaggle/working/submission.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13/3594239916.py:139: DeprecationWarning: The argument `min_periods` for `Expr.rolling_mean` is deprecated. It has been renamed to `min_samples`.\n",
      "  pl.col(col).rolling_mean(window_size=w, min_periods=1).alias(rm_name)\n",
      "/tmp/ipykernel_13/3594239916.py:147: DeprecationWarning: The argument `min_periods` for `Expr.rolling_std` is deprecated. It has been renamed to `min_samples`.\n",
      "  pl.col(col).rolling_std(window_size=w, min_periods=1).alias(vol_name)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. START KAGGLE EVALUATION SERVER\n",
    "# ============================================================\n",
    "\n",
    "inference_server = kei.DefaultInferenceServer(predict)\n",
    "\n",
    "if IS_COMP_RERUN:\n",
    "    # Mode ini dipakai saat SUBMIT notebook ke kompetisi\n",
    "    print(\"Detected competition rerun environment. Starting inference server...\")\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    # Mode lokal / saat run manual di Notebook\n",
    "    print(\"Running in LOCAL GATEWAY mode for debugging...\")\n",
    "    print(f\"Using input dir: {INPUT_DIR}\")\n",
    "    inference_server.run_local_gateway((str(INPUT_DIR),))\n",
    "\n",
    "    # Setelah lokal gateway selesai, cek apakah submission.parquet sudah dibuat\n",
    "    sub_path = Path(\"submission.parquet\")\n",
    "    if sub_path.exists():\n",
    "        print(\"\\nsubmission.parquet generated at:\", sub_path.resolve())\n",
    "    else:\n",
    "        print(\"\\nWARNING: submission.parquet not found in working directory.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14348714,
     "sourceId": 111543,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.441138,
   "end_time": "2025-12-04T14:18:12.672440",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T14:17:43.231302",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
